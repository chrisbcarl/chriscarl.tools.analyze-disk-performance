# encoding: utf-8
'''
The MIT License (MIT)
Copyright © 2023 Chris Carl <chrisbcarl@outlook.com>
Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the “Software”), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all
  copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.

Author:     Chris Carl <chrisbcarl@outlook.com>
Date:       2024-09-26
Modified:   2024-09-26

Modified:
    2025-08-07 - chrisbcarl - added create_partitions, delete_partitions
                              added health, now I just need to test
                              fine tuning after running it successfully, added defaults awareness
                              linting and re-org
    2025-08-06 - chrisbcarl - nearly full re-write, re-orged into app.py, will be further developed on
    2025-08-05 - chrisbcarl - moved files to the root of the perf filepath so the files
                                dont contend with each other for bandwidth or space
    2025-08-03 - chrisbcarl - moved functions around, perfected the read_bytearray_from_disk
                              cleaned up perf+fill+read minutia
    2025-08-02 - chrisbcarl - added perf+fill+read and smartmon
    2025-08-01 - chrisbcarl - changed to chriscarl.tools.analyze-disk-performance
                              created the 'health' module which is designed to rip through all disks
                              added crystaldiskinfo utilization, we'll see how it goes after testing.
    2024-09-28 - chrisbcarl - added "write" mode, much easier.
    2024-09-26 - chrisbcarl - complete rewrite with different modes this time
    2023-07-05 - chrisbcarl - init commit, behaves like sequential write/readback, thats it.

TODO:
    - have create be its own separate thing
        - have eveyrthing be its own thing.
        - define some common argument names, and within their OWN function, they can be --duration
            - during flow, they become --write_full-duration or --write_burn duration
    - separate write into write_full and write_burn, it'll just be easier that way.
    - deal with the situation where create_bytearray would result in a bytearray the size larger than the universe.
        better would be to make some object that when you ask for an index or the next byte,
        it GENRATES it, is writable, etc...
    - argparse flow it would be nice to have a dedicated "required" group but hey.

Examples:
    - multi-drive
    ... # typical: take all drives, give them a partition, benchmark it, write-read, repeat 3x, S.M.A.R.T. monitoring
    >>> python main.py health

    - writing
    ... # write fulpak w/ a 1mb file of 69s to fill the drive, do 2 fulpak writes
    >>> python main.py write --size 1mb --value 69 --data-filepath I:/tmp --iterations 2
    ... # write burnin w/ a 1kb file of 69s for 3 minutes
    >>> python main.py write --size 1024 --value 69 --data-filepath Y:/tmp --iterations 10 --burn-in
    ... # write fulpak w/ an autogenerated file indefinitely
    >>> python main.py write --data-filepath Y:/tmp --search-optimal

    - reading
    ... # create a random file that writes very quickly
    >>> python main.py write --data-filepath C:/temp/tmp --iterations 1 --burn-in --search-optimal --skip-telemetry
    ... # create a file of 10mb
    >>> python main.py write --data-filepath C:/temp/tmp --size 10mb --iterations 1 --burn-in --skip-telemetry
    ... # read the file sequentially 4 times
    >>> python main.py read --data-filepath C:/temp/tmp --iterations 4 --skip-telemetry
    ... # read the file randomly forever, the randomness of the array jumps around in 256 windows, notify every 4 gbs
    >>> python main.py read --data-filepath C:/temp/tmp --random-read 1mb --skip-telemetry --log-unit GB --log-mod 4

    - flow: combine multiple stuff
    ... # read then write then telemetry without the asynchronous thread
    >>> python main.py flow --steps write read telemetry --burn-in --size 1kb --flow-iterations 1

    - flows
    - What most people think of as write-read benchmarking
        python main.py perf+fill+read --data-filepath Y:/temp
    - Find performance sweetspot and fill the disk at partition D:/
        python main.py perf+fill --data-filepath D:/temp
    - Evaluate overall health on all newly inserted disks
        python main.py health --ignore-partitions C --log-level DEBUG
    - Just launch a cyrstaldiskinfo monitor
        python main.py smartmon
'''
# stdlib
from __future__ import print_function, division
import os
import sys
import csv
import copy
import time
import json
import pprint
import string
import random
import logging
import inspect
import argparse
import datetime
import threading
import subprocess
from typing import Tuple, Dict, List, Optional  # noqa: F401

# 3rd party
import numpy as np
import pandas as pd
import psutil

# app imports
import lib
import constants
from stdlib import NiceFormatter, abspath, touch

SCRIPT_DIRPATH = os.path.abspath(os.path.dirname(__file__))


def _telemetry_smart(event=threading.Event()):
    # type: (threading.Event) -> Tuple[Dict[str, dict], dict]
    '''
    Basically smart can fail to detect drive letter stuff from time to time, best to wait a while...
    '''
    # Traceback (most recent call last):
    #   File "C:\Python312\Lib\threading.py", line 1075, in _bootstrap_inner
    #     self.run()
    #   File "C:\Python312\Lib\threading.py", line 1012, in run
    #     self._target(*self._args, **self._kwargs)
    #   File "X:\src\chriscarl.tools.analyze-disk-performance\app.py", line 127, in _telemetry
    #     disk_number = letter_map[drive_letter]
    #                   ~~~~~~~~~~^^^^^^^^^^^^^^
    while True:
        try:
            cdi = lib.crystaldiskinfo()
            letter_map = {value['Drive Letter']: num for num, value in cdi.items()}
            return cdi, letter_map
        except Exception:
            logging.debug('error, trying again in 5 sec...', exc_info=True)
            for _ in range(int(5 * 100)):
                time.sleep(1 / 100)
                if event.is_set():
                    break


def summarize_crystaldiskinfo_df(df):
    df['datetime'] = pd.to_datetime(df['datetime'])
    df['Read Perf'] = pd.Series([''] * len(df))
    df['Write Perf'] = pd.Series([''] * len(df))
    ilocs = []
    rows = []
    for grouping, group_df in df.groupby(['Serial Number']):
        dts = group_df['datetime']
        dt_min, dt_max = dts.min(), dts.max()
        iloc_min = dts.index[dts.argmin()]
        iloc_max = dts.index[dts.argmax()]
        ilocs += [iloc_min, iloc_max]
        elapsed = (dt_max - dt_min).total_seconds()

        reads = group_df['Host Reads']
        reads_min, reads_max = reads.min(), reads.max()
        read_throughput, write_throughput = '?', '?'
        if reads_min == np.nan:
            reads_min = -1
            reads_min = -1
        elif isinstance(reads_min, (int, float)):
            pass
        else:
            unit = reads_min.split()[-1]
            reads_min, reads_max = reads_min.split()[0], reads_max.split()[0]
        reads_min, reads_max = float(reads_min), float(reads_max)

        writes = group_df['Host Writes']
        writes_min, writes_max = writes.min(), writes.max()
        if writes_min == np.nan:
            writes_min = -1
            writes_max = -1
        elif isinstance(writes_min, (int, float)):
            pass
        else:
            writes_min, writes_max = writes_min.split()[0], writes_max.split()[0]
        writes_min, writes_max = float(writes_min), float(writes_max)

        if elapsed == 0:
            read_throughput = f'0 {unit}/s'
            write_throughput = f'0 {unit}/s'
        else:
            read_throughput = f'{(reads_max - reads_min) / elapsed:0.3f} {unit}/s'
            write_throughput = f'{(writes_max - writes_min) / elapsed:0.3f} {unit}/s'
        df.loc[group_df.index, 'Read Perf'] = read_throughput
        df.loc[group_df.index, 'Write Perf'] = write_throughput

        drive_letter = df.iloc[iloc_max]['Drive Letter']
        disk_size = df.iloc[iloc_max]['Disk Size'].split()
        disk_size, disk_unit = disk_size[0], disk_size[1]
        serial = df.iloc[iloc_max]['Serial Number']
        health = df.iloc[iloc_max]['Health Status']
        disk_number = df.iloc[iloc_max]['Disk Number']
        pcs = df.iloc[iloc_max]['Power On Count'].split()[0]
        xfer = df.iloc[iloc_max]['Transfer Mode'].split(' | ')[-1]  # pcie
        # text = (
        #     f'{serial} ({disk_size} {disk_unit}) | {disk_number} | {drive_letter} | '
        #     f'{health} | Reads: {read_throughput} | Writes: {read_throughput} | '
        # )
        row = dict(
            serial=serial,
            size=f'{disk_size} {disk_unit}',
            number=disk_number,
            letter=drive_letter,
            health=health,
            elapsed=f'{elapsed / 3600:0.2f}hrs',
            reads=f'{reads_max - reads_min} {disk_unit}',
            writes=f'{writes_max - writes_min} {disk_unit}',
            read_bw=read_throughput,
            write_bw=write_throughput,
            pcs=pcs,
            pcie=xfer,
        )
        for col in constants.CRYSTAL_ERROR_KEYS:
            if col not in df.columns:
                continue
            errs = group_df[col]
            err_min, err_max = errs.min(), errs.max()
            if pd.isna(err_min):
                err_min = 0
            if pd.isna(err_max):
                err_max = 0
            err_delta = err_max - err_min
            row[f'{col} (Delta)'] = err_delta

        rows.append(row)

    df = pd.DataFrame(rows)
    return df


def telemetry_async(
    no_telemetry=constants.NO_TELEMETRY,
    no_admin=constants.NO_ADMIN,
    poll=constants.POLL,
    smart_filepath=constants.SMART_FILEPATH,
    data_filepath=constants.DATA_FILEPATH,
    summary_filepath=constants.SUMMARY_FILEPATH,
    no_crystaldiskinfo=constants.NO_CRYSTALDISKINFO,
    all_drives=constants.ALL_DRIVES,
    event=threading.Event(),
):
    # type: (bool, bool, float|int, str, str, str, bool, bool, threading.Event) -> None
    '''
    Description:
        Poll telemetry including S.M.A.R.T. and others.

    Arguments:
        no_telemetry: bool
            short circuit exit
        poll: float|int
            interval between sampling
        data_filepath: str
            the destination of the actual file to be written since we're operating at the OS level
        smart_filepath: str
            where to save the crystaldisinfo S.M.A.R.T. data
        summary_filepath: str
            where to save the final executive summary
        no_crystaldiskinfo: bool
            default False, disable so you can run without admin
        all_drives: bool
            default False, get all drive S.M.A.R.T. data instead of only the drive who hosts the data_filepath

    Returns:
        bytearray
    '''
    if no_telemetry:
        logging.warning('skipping telemetry!')
        return

    logging.debug('polling every %s sec', poll)
    if all_drives:
        drive_letter, _ = os.path.splitdrive(data_filepath)
    else:
        drive_letter = ''

    prior_percent = None
    disk_number = ''

    if not no_admin and not no_crystaldiskinfo:
        cdi, letter_map = _telemetry_smart(event=event)
        logging.debug(pprint.pformat(letter_map, indent=2))
        if not cdi:
            # we failed and tried multiple times or things were cancelled early during a failure
            return
        cdi_df = pd.DataFrame(cdi.values())
        summary_columns = [key for key in constants.CRYSTAL_KEYS if key in cdi_df.columns]
        logging.debug('\n%s', cdi_df[summary_columns])
        if drive_letter:
            disk_number = letter_map[drive_letter]
        columns = upsert_df_to_csv(cdi_df, smart_filepath)

    iteration = 0
    while not event.is_set():
        for _ in range(int(poll * 1000)):
            time.sleep(1 / 1000)
            if event.is_set():
                break

        logging.debug('poll: %d', iteration)
        if not no_admin and no_crystaldiskinfo:
            cdi = lib.crystaldiskinfo()
            with open(smart_filepath, 'a', encoding='utf-8', newline='') as a:
                writer = csv.DictWriter(a, fieldnames=columns)
                if disk_number:
                    value = cdi[str(disk_number)]
                    writer.writerow(value)
                    logging.debug('\n%s', pd.DataFrame([{k: v for k, v in value.items() if k in summary_columns}]))
                else:
                    for value in cdi.values():
                        writer.writerow(value)
                    logging.debug(
                        '\n%s',
                        pd.DataFrame(
                            [{
                                k: v
                                for k, v in value.items() if k in summary_columns
                            } for value in cdi.values()]
                        )
                    )

        if drive_letter and disk_number:
            du = psutil.disk_usage(drive_letter)
            if str(du.percent) != str(prior_percent):
                logging.info('disk usage (%s|disk %s): %s%%', drive_letter, disk_number, du.percent)
                prior_percent = str(du.percent)

        iteration += 1

    if not no_admin and not no_crystaldiskinfo:
        cdi = lib.crystaldiskinfo()
        if drive_letter and disk_number:
            df = pd.DataFrame([cdi[str(disk_number)]])
        else:
            df = pd.DataFrame(cdi.values())
        upsert_df_to_csv(df, smart_filepath)
        logging.info('S.M.A.R.T. Telemetry:\n%s', df)

        cdi_df = pd.read_csv(smart_filepath)

        summary_df = summarize_crystaldiskinfo_df(cdi_df)
        summary_df.to_csv(summary_filepath, index=False)


def telemetry(
    smart_filepath=constants.SMART_FILEPATH,
    data_filepath=constants.DATA_FILEPATH,
    summary_filepath=constants.SUMMARY_FILEPATH,
    no_crystaldiskinfo=constants.NO_CRYSTALDISKINFO,
    all_drives=constants.ALL_DRIVES,
):
    # type: (str, str, str, bool, bool) -> None
    '''
    Description:
        Poll telemetry including S.M.A.R.T. and others.

    Arguments:
        data_filepath: str
            the destination of the actual file to be written since we're operating at the OS level
        smart_filepath: str
            where to save the crystaldisinfo S.M.A.R.T. data
        summary_filepath: str
            where to save the final executive summary
        no_crystaldiskinfo: bool
            default False, disable so you can run without admin
        all_drives: bool
            default False, get all drive S.M.A.R.T. data instead of only the drive who hosts the data_filepath

    Returns:
        bytearray
    '''
    if all_drives:
        drive_letter, _ = os.path.splitdrive(data_filepath)
    else:
        drive_letter = ''

    disk_number = ''

    if not no_crystaldiskinfo:
        cdi, letter_map = _telemetry_smart()
        logging.debug(pprint.pformat(letter_map, indent=2))
        if not cdi:
            # we failed and tried multiple times or things were cancelled early during a failure
            return
        cdi_df = pd.DataFrame(cdi.values())
        summary_columns = [key for key in constants.CRYSTAL_KEYS if key in cdi_df.columns]
        logging.debug('\n%s', cdi_df[summary_columns])
        if drive_letter:
            disk_number = letter_map[drive_letter]
        columns = upsert_df_to_csv(cdi_df, smart_filepath)

        with open(smart_filepath, 'a', encoding='utf-8', newline='') as a:
            writer = csv.DictWriter(a, fieldnames=columns)
            for value in cdi.values():
                writer.writerow(value)

        if drive_letter and disk_number:
            df = pd.DataFrame([cdi[str(disk_number)]])
        else:
            df = pd.DataFrame(cdi.values())
        logging.info('S.M.A.R.T. Telemetry:\n%s', df)

        summary_df = summarize_crystaldiskinfo_df(cdi_df)
        summary_df.to_csv(summary_filepath, index=False)

    if drive_letter and disk_number:
        du = psutil.disk_usage(drive_letter)
        logging.info('disk usage (%s|disk %s): %s%%', drive_letter, disk_number, du.percent)


def crystaldiskinfo_detect():
    # type: () -> int
    try:
        output = subprocess.check_output(
            ['where.exe' if sys.platform == 'win32' else 'which', constants.CRYSTALDISKINFO_EXE],
            universal_newlines=True
        )
        for line in output.splitlines():
            strip = line.strip()
            if os.path.isfile(strip):
                constants.CRYSTALDISKINFO_EXE = strip
                break
        if not os.path.isfile(strip):
            raise OSError(f'Could not find "{constants.CRYSTALDISKINFO_EXE}"!')
    except subprocess.CalledProcessError:
        logging.warning('CrystalDiskInfo not installed or not on path!')
        return 1
    return 0


def admin_detect():
    # type: () -> int
    try:
        admin_ps1 = abspath(SCRIPT_DIRPATH, r"scripts\win32\admin.ps1")
        subprocess.check_call(['powershell', admin_ps1])
        logging.info('admin detected!')
    except subprocess.CalledProcessError:
        logging.warning('not admin!')
        return 1
    return 0


def telemetry_thread(
    no_telemetry=constants.NO_TELEMETRY,
    no_admin=constants.NO_ADMIN,
    poll=constants.POLL,
    smart_filepath=constants.SMART_FILEPATH,
    data_filepath=constants.DATA_FILEPATH,
    summary_filepath=constants.SUMMARY_FILEPATH,
    no_crystaldiskinfo=constants.NO_CRYSTALDISKINFO,
    all_drives=constants.ALL_DRIVES,
):
    # type: (bool, bool, float|int, str, str, str, bool, bool) -> Tuple[Optional[threading.Event], Optional[threading.Thread]]  # noqa: E501
    if no_telemetry:
        logging.warning('skipping telemetry!')
        return None, None

    logging.debug('checking admin access...')
    if admin_detect() != 0:
        if no_admin:
            logging.warning('running without admin privaleges, setting crystaldiskinfo low!')
            no_crystaldiskinfo = True
        else:
            raise RuntimeError('Must be run as administrator or sudo!')

    if not no_crystaldiskinfo:
        logging.debug('checking CrystalDiskInfo access...')
        if crystaldiskinfo_detect() != 0:
            raise RuntimeError('Cannot run CrystalDiskInfo!')

    event = threading.Event()
    t = threading.Thread(
        target=telemetry_async,
        kwargs=dict(
            no_telemetry=no_telemetry,
            no_admin=no_admin,
            event=event,
            smart_filepath=smart_filepath,
            data_filepath=data_filepath,
            summary_filepath=summary_filepath,
            poll=poll,
            no_crystaldiskinfo=no_crystaldiskinfo,
            all_drives=all_drives,
        )
    )
    t.start()
    return event, t


def upsert_df_to_csv(df, filepath, index=False):
    # type: (pd.DataFrame, str, bool) -> List[str]
    dirpath = os.path.dirname(filepath)
    os.makedirs(dirpath, exist_ok=True)
    if os.path.isfile(filepath):
        old_df = pd.read_csv(filepath)
        new = pd.concat([old_df, df])
        new.to_csv(filepath, index=index)
        return new.columns.tolist()
    else:
        df.to_csv(filepath, index=index)
        return df.columns.tolist()


def fill_till_bursting(
    byte_array,
    drive,
    data_filepath=constants.DATA_FILEPATH,
    event=threading.Event(),
    log_unit=constants.LOG_UNIT,
    log_mod=constants.LOG_MOD,
):
    # type: (bytearray, str, str, threading.Event, str, int) -> None
    # write the bulk of the data
    unit = constants.LOG_UNITS[log_unit]
    size = len(byte_array)
    prior = ''
    with open(data_filepath, 'ab') as wb:
        while psutil.disk_usage(drive).free > size:
            if event.is_set():
                return
            wb.write(byte_array)

            getsize = os.path.getsize(data_filepath) / unit
            getsizestr = str(int(getsize))
            if getsizestr != prior and int(getsize) % log_mod == 0:  # really slow it down
                logging.info('%0.3f %s written', getsize, log_unit)
                prior = getsizestr

        try:
            # write the last chunk in 1mb increments until disk fills and raises OSError
            for i in range(size // constants.MB):
                if event.is_set():
                    return
                if psutil.disk_usage(drive).free > constants.MB:
                    one_mb_array = byte_array[i * constants.MB:(i + 1) * unit]
                    wb.write(one_mb_array)
                else:
                    break
        except OSError:
            pass  # this is expected behavior

    remainder = size - os.path.getsize(data_filepath)
    wb.write(byte_array[0:remainder])
    getsize = os.path.getsize(data_filepath) / unit
    logging.info('%0.3f %s written', getsize, log_unit)


def write(
    data_filepath=constants.DATA_FILEPATH,
    size=constants.SIZE,
    value=constants.VALUE,
    search_optimal=constants.SEARCH_OPTIMAL,
    search_optimal_filepath=constants.SEARCH_OPTIMAL_FILEPATH,
    iterations=constants.ITERATIONS,
    duration=constants.DURATION,
    burn_in=constants.BURN_IN,
    byte_array=None,
    log_unit=constants.LOG_UNIT,
    log_mod=constants.LOG_MOD,
    delete=constants.DELETE,
):
    # type: (str, int, int, bool, str, int, float|int, bool, Optional[bytearray], str, int, bool) -> bytearray
    '''
    Description:
        Write a file to the disk, perhaps random, repeatedly, fill the drive, set size, etc.

        >>> write('/tmp/file')  # write random file to fill the disk, loop indefinitely
        >>> write('/tmp/file', duration=10)  # write random file to fill the disk, break if elapsed exceeded, else loop
        >>> write('/tmp/file', value=69, size=1024)  # write 1024 bytes of 64 to the same file infinitely (burn-in)

    Arguments:
        data_filepath: str
            the destination of the actual file to be written since we're operating at the OS level
        size: int
            -1 to fill the disk, else, size in in bytes
            negates search_optimal
        value: int
            -1 for random, else, repeat the same value for all bytes
        search_optimal_filepath: bool
            default true (fill the disk). if true, do some repeated writes to find the "best" size, else just generate
        iterations: int
            -1 for loop infinitely, else exec ends iteration exceeded (or duration exceeded)
        duration: float|int
            -1 for loop infinitely, else exec ends after elapsed exceeded in seconds (or duration exceeded)
        burn_in: bool
            default False, rewrite to the same place, not append and fill
        byte_array: Optional[bytearray]
            use this bytearray instead of generating it (useful when passing one to the other)
        log_unit: str
            default GB, frequency of i/o that gets logged by units
        log_mod: int
            default 8, frequency of i/o that gets logged by EVERY OTHER units
        delete: bool
            default False, after operation, self-cleanup

    Returns:
        bytearray
    '''
    drive_letter, _ = os.path.splitdrive(data_filepath)
    if search_optimal:
        logging.info('creating bytearray...')
        byte_array = lib.create_byte_array_high_throughput(
            data_filepath=data_filepath, search_optimal_filepath=search_optimal_filepath, value=value
        )
    create = False
    if isinstance(byte_array, bytearray):
        if len(byte_array) == 0:
            create = True
        logging.info('reusing bytearray of size %s...', lib.bytes_to_size(len(byte_array)))
    else:
        create = True
    if create:
        logging.info('creating bytearray...')
        if size != constants.SIZE:
            byte_array = lib.create_bytearray(size, value=value)
        else:
            byte_array = lib.create_byte_array_high_throughput(
                data_filepath=data_filepath, search_optimal_filepath=search_optimal_filepath, value=value
            )
    byte_array = byte_array or bytearray()  # for type hinting

    filesize = lib.bytes_to_size(len(byte_array))
    until = ''
    if iterations == constants.ITERATIONS and duration == constants.DURATION:
        until = 'infinitely'
    elif duration != constants.DURATION:
        until = f'{duration} secs'
    elif iterations != constants.ITERATIONS:
        until = f'{iterations} iterations'
    summary = (
        f'writing in {"burn-in (write)" if burn_in else "fulpak (append)"} mode, '
        f'on drive {drive_letter}, '
        f'repeatedly using a {filesize} file{"" if burn_in else " in chunks"}, '
        f'running {until}'
    )
    logging.warning(summary)

    try:
        touch(data_filepath)
        iteration = 1
        if iterations > -1:
            for iteration in range(1, iterations + 1):
                logging.info('writing %d / %d', iteration, iterations)
                touch(data_filepath)
                if burn_in:
                    with open(data_filepath, 'wb') as wb:
                        wb.write(byte_array)
                else:
                    fill_till_bursting(
                        byte_array,
                        drive_letter,
                        data_filepath=data_filepath,
                        log_unit=log_unit,
                        log_mod=log_mod,
                    )
                if delete:
                    touch(data_filepath)

        elif duration > -1:
            start = time.time()
            elapsed = time.time() - start
            while elapsed < duration:
                logging.info('writing until %s > %s, iteration %d', elapsed, duration, iteration)
                touch(data_filepath)
                if burn_in:
                    with open(data_filepath, 'wb') as wb:
                        wb.write(byte_array)
                else:
                    # TODO: change architecture to duration and timer based, when the timer goes off,
                    #   an event is flagged, signalling everyone to halt the fuck up
                    fill_till_bursting(
                        byte_array,
                        drive_letter,
                        data_filepath=data_filepath,
                        log_unit=log_unit,
                        log_mod=log_mod,
                    )
                if delete:
                    touch(data_filepath)
                if elapsed > duration:
                    logging.info('time exceeded,  %s > %s', elapsed, duration)
                    break

                iteration += 1
                elapsed = time.time() - start
        else:
            while True:
                logging.info('writing infinitely, iteration %d', iteration)
                touch(data_filepath)
                if burn_in:
                    with open(data_filepath, 'wb') as wb:
                        wb.write(byte_array)
                else:
                    fill_till_bursting(
                        byte_array,
                        drive_letter,
                        data_filepath=data_filepath,
                        log_unit=log_unit,
                        log_mod=log_mod,
                    )
                if delete:
                    touch(data_filepath)

                iteration += 1
    except KeyboardInterrupt:
        logging.warning('cancelling...')
    finally:
        if delete:
            touch(data_filepath)
        return byte_array


def read_random(
    byte_array,
    data_filepath=constants.DATA_FILEPATH,
    chunk_size=16,
    log_unit=constants.LOG_UNIT,
    log_mod=constants.LOG_MOD,
):
    # type: (bytearray, str, int, str, int) -> int
    '''
    Description:
        Read a file by randomly jumping around with seek and reads

    Arguments:
        byte_array: Optional[bytearray]
            use this bytearray instead of generating it (useful when passing one to the other)
        data_filepath: str
            the destination of the actual file to be written since we're operating at the OS level
        chunk_size: int
            default -1, MUST evenly divide byte_array length
                instead of sequentially asserting, randomly dart around the file
                say the byte_array length 32, data_filepath length 64, chunks 4
                we will generate 64 / 4 = 16 "windows" to jump around and compare

    Returns:
        int
            return the total bytes read
    '''
    unit = constants.LOG_UNITS[log_unit]
    filesize = os.path.getsize(data_filepath)
    arrsize = len(byte_array)
    if arrsize % chunk_size != 0:
        raise TypeError(
            f'chunks must evenly divide the byte_array size! {arrsize} % {chunk_size} == {arrsize % chunk_size}!'
        )
    # [0, 640, 1280, 1920, 2560, 3200, 3840, 4480, 5120, 5760, 6400, 7040, 7680, 8320, 8960, 9600]
    idxes = list(range(0, filesize, chunk_size))
    # [3840, 9600, 1920, 640, 5760, 4480, 2560, 3200, 5120, 7680, 7040, 6400, 8320, 0, 8960, 1280]
    random.shuffle(idxes)
    bytes_read = 0
    with open(data_filepath, 'rb') as rb:
        prior = ''
        for i, file_idx in enumerate(idxes):
            _ = rb.seek(file_idx)
            read_array = rb.read(chunk_size)

            truth_idx = file_idx % arrsize
            # in case we're at the LAST idx, and didnt read much
            truth_array = byte_array[truth_idx:truth_idx + len(read_array)]
            assert read_array == truth_array, (
                '\n'.join([f'on iteration {i}, full array read != write!'] + lib.diff_bytes(read_array, truth_array))
            )
            bytes_read += len(read_array)
            bytes_read_size = bytes_read / unit
            bytes_read_size_str = str(int(bytes_read_size))
            # print(bytes_read)
            if prior != bytes_read_size_str and int(bytes_read_size) % log_mod == 0:  # really slow it down
                logging.debug('%0.3f%% or %0.3f %s read', (i + 1) / (len(idxes)) * 100, bytes_read_size, log_unit)
                prior = bytes_read_size_str

    bytes_read_size = bytes_read / unit
    bytes_read_size_str = f'{bytes_read_size:0.2f}'
    logging.debug('%0.3f%% or %0.3f %s read', (i + 1) / (len(idxes)) * 100, bytes_read_size, log_unit)
    prior = bytes_read_size_str

    return bytes_read


def read(
    byte_array=None,
    data_filepath=constants.DATA_FILEPATH,
    random_read=constants.RANDOM_READ,
    iterations=constants.ITERATIONS,
    duration=constants.DURATION,
    log_unit=constants.LOG_UNIT,
    log_mod=constants.LOG_MOD,
):
    # type: (Optional[bytearray], str, int, int, float|int, str, int) -> bool
    '''
    Description:
        Read a file from the disk, perhaps random, repeatedly, etc.

        >>> ba, data_filepath = bytearray(list(range(1024))), '/tmp/file'
        >>> read(byte_array=ba, data_filepath=data_filepath)  # window-read file sequentially, assert match
        >>> read(byte_array=ba, data_filepath=data_filepath, random_read=True)  # generate random windows, assert match

    Arguments:
        byte_array: Optional[bytearray]
            use this bytearray instead of generating it (useful when passing one to the other)
        data_filepath: str
            the destination of the actual file to be written since we're operating at the OS level
        random_read: int
            default -1, MUST evenly divide byte_array length
                instead of sequentially asserting, randomly dart around the file
                say the byte_array length 32, data_filepath length 64, chunks 4
                we will generate 64 / 4 = 16 "windows" to jump around and compare
        iterations: int
            -1 for loop infinitely, else exec ends iteration exceeded (or duration exceeded)
        duration: float|int
            -1 for loop infinitely, else exec ends after elapsed exceeded in seconds (or duration exceeded)
        log_unit: str
            default GB, frequency of i/o that gets logged by units
        log_mod: int
            default 8, frequency of i/o that gets logged by EVERY OTHER units

    Returns:
        bool
    '''
    unit = constants.LOG_UNITS[log_unit]
    if byte_array is None and os.path.isfile(data_filepath):
        logging.info('loading bytearray from data_filepath at "%s"', data_filepath)
        with open(data_filepath, 'rb') as rb:
            byte_array = bytearray(rb.read())
    if not isinstance(byte_array, bytearray):
        raise TypeError(f'byte_array must be of type bytearray, provided {type(byte_array)}!')
    filesize = os.path.getsize(data_filepath)
    arrsize = len(byte_array)
    if random_read != constants.RANDOM_READ and arrsize % random_read != 0:
        raise TypeError(
            f'random_read must evenly divide the byte_array size! {arrsize} % {random_read} == {arrsize % random_read}!'
        )

    summary = "random " if random_read != constants.RANDOM_READ else ""
    chunk_count = filesize / random_read
    logging.warning(
        '%sread of file of %s by assertion array of %s in %s chunks', summary, lib.bytes_to_size(filesize),
        lib.bytes_to_size(arrsize), chunk_count
    )

    prior = ''
    total_bytes_read = 0
    iteration = 1
    if iterations > -1:
        for iteration in range(1, iterations + 1):
            logging.info('reading %d / %d', iteration, iterations)
            if random_read == constants.RANDOM_READ:
                bytes_read = lib.read_bytearray_from_disk(
                    byte_array,
                    data_filepath=data_filepath,
                    log_unit=log_unit,
                    log_mod=log_mod,
                )
            else:
                bytes_read = read_random(
                    byte_array,
                    data_filepath=data_filepath,
                    chunk_size=random_read,
                    log_unit=log_unit,
                    log_mod=log_mod,
                )
            total_bytes_read += bytes_read

            logsize = total_bytes_read / unit
            logsizestr = str(int(logsize))
            if prior != logsizestr and int(logsize) % log_mod == 0:  # really slow it down
                logging.info('%0.3f%% or %0.3f %s read', iteration / (iterations) * 100, logsize, log_unit)
                prior = logsizestr

    elif duration > -1:
        start = time.time()
        elapsed = time.time() - start
        while elapsed < duration:
            logging.info('reading until %s > %s, iteration %d', elapsed, duration, iteration)
            if random_read == constants.RANDOM_READ:
                bytes_read = lib.read_bytearray_from_disk(
                    byte_array,
                    data_filepath=data_filepath,
                    log_unit=log_unit,
                    log_mod=log_mod,
                )
            else:
                bytes_read = read_random(
                    byte_array,
                    data_filepath=data_filepath,
                    chunk_size=random_read,
                    log_unit=log_unit,
                    log_mod=log_mod,
                )
            total_bytes_read += bytes_read

            logsize = total_bytes_read / unit
            logsizestr = str(int(logsize))
            if prior != logsizestr and int(logsize) % log_mod == 0:  # really slow it down
                logging.info('%0.3f%% or %0.3f %s read', elapsed / (duration) * 100, logsize, log_unit)
                prior = logsizestr

            iteration += 1
            elapsed = time.time() - start

    else:
        while True:
            logging.debug('reading infinitely, iteration %d', iteration)
            if random_read == constants.RANDOM_READ:
                bytes_read = lib.read_bytearray_from_disk(
                    byte_array,
                    data_filepath=data_filepath,
                    log_unit=log_unit,
                    log_mod=log_mod,
                )
            else:
                bytes_read = read_random(
                    byte_array,
                    data_filepath=data_filepath,
                    chunk_size=random_read,
                    log_unit=log_unit,
                    log_mod=log_mod,
                )
            total_bytes_read += bytes_read

            logsize = total_bytes_read / unit
            logsizestr = str(int(logsize))
            if prior != logsizestr and int(logsize) % log_mod == 0:  # really slow it down
                logging.info('inf%% or %0.3f %s read', logsize, log_unit)
                prior = logsizestr

            iteration += 1

    return True


def delete_partitions(ignore_partitions=constants.IGNORE_PARTITIONS, include_partitions=None):
    # type: (List[str], Optional[List[str]]) -> List[str]
    '''
    Description:
        Look through all partitions and remove them so the disks are raw

    Arguments:
        ignore_partitions: List[str]
            Ex) ['C']
            If you know of partitions youd like to avoid ahead of time, maybe avoid deleting them...
        include_partitions: List[str]
            If you know which to delete, delete only those, override ignore_partitions

    Returns:
        List[str]
            list of disk numbers as readable by CrystalDiskMark and Windows Disk Utility
    '''
    if admin_detect() != 0:
        raise RuntimeError('Must be run as administrator or sudo!')

    disk_numbers = []
    if not include_partitions:
        # get all partitions
        read_partitions_ps1 = abspath(SCRIPT_DIRPATH, r"scripts\win32\read-partitions.ps1")
        cmd = ['powershell', read_partitions_ps1]
        logging.debug(subprocess.list2cmdline(cmd))
        output = subprocess.check_output(cmd, universal_newlines=True)
        read_partitions = json.loads(output)
        logging.info('partitions identified: %s', json.dumps(read_partitions, indent=2))

        # filter out all partitions that dont belong
        include_partitions = [key for key in read_partitions if key not in ignore_partitions]
        logging.debug('removing drive letters: %s', include_partitions)
        disk_numbers = [val['DiskNumber'] for key, val in read_partitions.items() if key not in ignore_partitions]
        logging.info('disk numbers to be removed after filtering %s: %s', ignore_partitions, disk_numbers)

    if include_partitions:
        # remove partitions so they return to raw
        delete_partitions_ps1 = abspath(SCRIPT_DIRPATH, r"scripts\win32\delete-partitions.ps1")
        cmd = ['powershell', delete_partitions_ps1, '-DriveLetters', ','.join(include_partitions)]
        logging.debug(subprocess.list2cmdline(cmd))
        output = subprocess.check_output(cmd, universal_newlines=True)
        logging.debug(output)
    if disk_numbers:
        logging.info('deleted partitions. disk numbers: %s', disk_numbers)
        return disk_numbers

    return disk_numbers


def create_partitions(disk_numbers=constants.DISK_NUMBERS):
    # type: (List[str|int]) -> Dict[str, str]
    '''
    Description:
        Go through any raw disk and give them a partition, returning a dict of number: letter

    Arguments:
        disk_numbers: List[int]
            Ex) [0]
            If you know the disk numbers ahead of time, provide them
            else, any raw disk will be partitioned and assigned a drive letter

    Returns:
        Dict[str, str]
            dict of number: letter
            ex) {"1": "D:"}
    '''
    if admin_detect() != 0:
        raise RuntimeError('Must be run as administrator or sudo!')

    # scan for RAW and make new partitions
    create_partitions_ps1 = abspath(SCRIPT_DIRPATH, r"scripts\win32\create-partitions.ps1")
    cmd = ['powershell', create_partitions_ps1]
    if disk_numbers:
        cmd += ['-DiskNumbers', ','.join(str(ele) for ele in disk_numbers)]
    logging.debug(subprocess.list2cmdline(cmd))
    output = subprocess.check_output(cmd, universal_newlines=True)
    create_partitions_sentinel = "Begin Output Parsing Here:"
    logging.debug(output)
    output = output[output.find(create_partitions_sentinel) + len(create_partitions_sentinel) + 1:].strip()
    logging.debug(output)
    disk_number_to_letter_dict = json.loads(output)
    logging.info('created new partitions: %s!', disk_number_to_letter_dict)
    for k, v in disk_number_to_letter_dict.items():
        if v is None:
            raise RuntimeError(f'Drive {k} was unable to create a partition: {v}!')

    return disk_number_to_letter_dict


def health(
    # delete
    ignore_partitions=constants.IGNORE_PARTITIONS,
    include_partitions=None,
    # array
    size=constants.SIZE,
    value=constants.VALUE,
    search_optimal_filepath=constants.SEARCH_OPTIMAL_FILEPATH,
    # flow
    iterations=3,
    duration=constants.DURATION,
    # write
    burn_in=constants.BURN_IN,
    # read
    random_read=constants.RANDOM_READ,
    # general/telemetry
    log_level=constants.LOG_LEVEL,
    log_unit=constants.LOG_UNIT,
    log_mod=constants.LOG_MOD,
    poll=constants.POLL,
):
    '''
    Description:
        Launch a pre-determined flow upon every relevant disk. WARNING: DO NOT RUN IN HIGHLY POPULATED PCs!

    Arguments:
        log_unit: str
            default GB, frequency of i/o that gets logged by units
        log_mod: int
            default 8, frequency of i/o that gets logged by EVERY OTHER units
    '''
    operation = 'write+read'
    logging.info('starting %r', operation)

    if admin_detect() != 0:
        raise RuntimeError('Must be run as administrator or sudo!')

    logging.info('deleting partitions...')
    disk_numbers = delete_partitions(ignore_partitions=ignore_partitions, include_partitions=include_partitions)

    logging.info('creating partitions...')
    disk_number_to_letter_dict = create_partitions(disk_numbers=disk_numbers)
    if disk_numbers and (len(disk_numbers) != len(disk_number_to_letter_dict)):
        raise RuntimeError(
            f'Number of disks != number of partitions created! '
            f'{len(disk_numbers)} != {len(disk_number_to_letter_dict)} | {disk_number_to_letter_dict}'
        )

    popens = []
    logging.info('flowing on %d partitions/drives...', len(disk_number_to_letter_dict))
    started = datetime.datetime.now()
    output_dirpath = constants.TEMP_DIRPATH
    for drive_number, drive_letter in disk_number_to_letter_dict.items():
        data_filepath = abspath(f'{drive_letter}:/{drive_number}-{operation}.dat')
        search_optimal_filepath = abspath(f'{output_dirpath}/{drive_number}-{operation}-search_optimal.csv')
        stdout = abspath(f'{output_dirpath}/{drive_number}-{operation}.stdout')
        cmd = [
            sys.executable,
            abspath(__file__),
            # flow control
            'flow',
            '--steps',
            'write',
            'read',
            '--flow-iterations',
            iterations,
            '--flow-duration',
            duration,
            # general / telemetry
            '--skip-telemetry',
            '--data-filepath',
            data_filepath,
            '--log-level',
            log_level,
            # array
            '--search-optimal-filepath',
            search_optimal_filepath,
            # read/write args
            '--iterations',
            1
        ]

        # array
        if size == constants.SIZE:
            cmd += ['--search-optimal']
        else:
            cmd += ['--size', size]
        if value != constants.VALUE:
            cmd += ['--value', value]

        if burn_in:
            cmd += ['--burn-in']
        if random_read != constants.RANDOM_READ:
            cmd += ['--random-read', random_read]
        cmd = [str(ele) for ele in cmd]
        logging.debug('drive %s (%s): %s', drive_number, drive_letter, subprocess.list2cmdline(cmd))
        with open(stdout, 'wb') as sout:
            popen = subprocess.Popen(cmd, stdout=sout)
            popens.append(popen)

    try:
        while True:
            now = datetime.datetime.now()
            logging.info('elapsed: %s', now - started)
            if all([popen.poll() is not None for popen in popens]):
                logging.info('All %r finished!', operation)
                break
            time.sleep(poll)
    except KeyboardInterrupt:
        logging.warning('ctrl + c detected! killing processes, removing resources...')

    logging.info('closing resources...')
    for popen in popens:
        if popen.poll() is None:
            popen.kill()
            subprocess.Popen(['taskkill', '/pid', str(popen.pid), '/f', '/t'], shell=True).wait()
    for drive_number, drive_letter in disk_number_to_letter_dict.items():
        data_filepath = abspath(f'{drive_letter}:/{drive_number}-{operation}.dat')
        if os.path.isfile(data_filepath):
            try:
                os.remove(data_filepath)
            except Exception:
                logging.error('unable to delete ""%s', data_filepath, exc_info=True)

    logging.info('removing partitions...')
    delete_partitions(include_partitions=list(disk_number_to_letter_dict.values()))


FUNC_NAMES = ['write', 'read', 'flow', 'telemetry', 'delete_partitions', 'create_partitions', 'health']


def flow_run(steps=FUNC_NAMES, **kwargs):
    '''
    Description:
        run other functions in a long line
    '''
    gbls = globals()
    for s, step in enumerate(steps):
        func = gbls[step]
        logging.info('starting %s / %s - %r', s + 1, len(steps), func.__name__)

        signature = inspect.signature(func)
        subkwargs = {k: kwargs[k] for k in signature.parameters if k in kwargs}
        # otherwise its too much to print
        logging.debug(pprint.pformat({k: v for k, v in subkwargs.items() if k not in ['byte_array']}, indent=2))

        res = func(**subkwargs)
        if isinstance(res, bytearray):
            kwargs['byte_array'] = res
        elif func == delete_partitions:
            kwargs['disk_numbers'] = res
        elif func == create_partitions:
            kwargs['disk_number_to_letter_dict'] = res


def flow(steps=FUNC_NAMES, flow_iterations=constants.ITERATIONS, flow_duration=constants.DURATION, **kwargs):
    '''
    Description:
        run other functions serially
    '''
    iteration = 1
    if flow_iterations > -1:
        for iteration in range(1, flow_iterations + 1):
            logging.info('flow %d / %d', iteration, flow_iterations)
            flow_run(steps=steps, **kwargs)

    elif flow_duration > -1:
        start = time.time()
        elapsed = time.time() - start
        while elapsed < flow_duration:
            logging.info('flow until %s > %s, iteration %d', elapsed, flow_duration, iteration)
            flow_run(steps=steps, **kwargs)

            iteration += 1
            elapsed = time.time() - start

    else:
        while True:
            logging.info('flow infinitely, iteration %d', iteration)
            flow_run(steps=steps, **kwargs)

            iteration += 1


_lcls = locals()
FUNCS = [_lcls[key] for key in FUNC_NAMES]
# i split them up so that during dynamic argparse parser creation they have separate groups
OPERATION_ARGUMENTS = {
    'data_filepath': dict(type=str, default=constants.DATA_FILEPATH, help='file that stresses the disk'),
    'size': dict(type=str, default=constants.SIZE, help='size in bytes, can use human friendly like 1024KB'),
    'value': dict(type=int, default=constants.VALUE, help='default random, fill with constant value'),
    'search_optimal': dict(type=bool, default=constants.SEARCH_OPTIMAL, help='test different sizes for throughput'),
    'search_optimal_filepath': dict(type=str, default=constants.SEARCH_OPTIMAL_FILEPATH, help='throughput csv'),
    'iterations': dict(type=int, default=constants.ITERATIONS, help='repetitions, -1 for infinitely'),
    'duration': dict(type=float, default=constants.DURATION, help='in seconds, -1 for infinitely'),
    'burn_in': dict(type=bool, help='default False, rewrite to the same place, not append and fill'),
    'steps': dict(type=str, nargs='+', choices=FUNC_NAMES, required=True, help='run funcs in series'),
    'random_read':
        dict(type=str, default=constants.RANDOM_READ, help='default -1, MUST evenly divide size, randomly dart around'),
    'flow_iterations': dict(type=int, default=constants.FLOW_ITERATIONS, help='repetitions, -1 for infinitely'),
    'flow_duration': dict(type=float, default=constants.FLOW_DURATION, help='in seconds, -1 for infinitely'),
    'byte_array': dict(type=bytearray, nargs='+', default=bytearray(), help='BUG: used as part of dynamic programming'),
    'ignore_partitions':
        dict(type=str, nargs='*', default=constants.IGNORE_PARTITIONS, help='if known partitions, ignore these'),
    'include_partitions': dict(type=str, nargs='*', default=[], help='override ignore, include only these'),
    'disk_numbers': dict(type=int, nargs='*', default=constants.DISK_NUMBERS, help='if known disk numbers, use these'),
    'disk_number_to_letter_dict': dict(type=str, help='pass as string, ex) {"1": "D:"}'),
    'log_unit': dict(type=str, default=constants.LOG_UNIT, choices=constants.LOG_UNITS, help='i/o log frequency'),
    'log_mod': dict(type=int, default=constants.LOG_MOD, help='i/o log frequency by EVERY OTHER --log_unit '),
    'delete': dict(type=bool, help='default False, after operation, self-cleanup'),
}
TELEMETRY_ARGUMENTS = {
    'all_drives': dict(type=bool, help='if enabled, it queries telemetry from all drives, rather than the one'),
    'poll': dict(type=float, default=constants.POLL, help='telemetry poll poll'),
    'no_telemetry': dict(type=bool, help='skip telemetry entirely'),
    'no_admin': dict(type=bool, help='do what you can without admin'),
    'no_crystaldiskinfo': dict(type=bool, help='if disabled, you can run without admin!'),
    'data_filepath': dict(type=str, default=constants.DATA_FILEPATH, help='file that stresses the disk'),
    'smart_filepath': dict(type=str, default=constants.SMART_FILEPATH, help='dump S.M.A.R.T. from CrystalDiskInfo.'),
    'summary_filepath': dict(type=str, default=constants.SUMMARY_FILEPATH, help='afteraction summary'),
    'log_level': dict(type=str, default=constants.LOG_LEVEL, choices=constants.LOG_LEVELS, help='log level'),
}
ARGUMENTS = {'operation': OPERATION_ARGUMENTS, 'telemetry': TELEMETRY_ARGUMENTS}


def validate_kwargs(
    args,
    func=write,
    # members of lists
    log_level=constants.LOG_LEVEL,
    log_unit=constants.LOG_UNITS,
    # numbers
    log_mod=constants.LOG_MOD,
    value=constants.VALUE,
    size=constants.SIZE,
    iterations=constants.ITERATIONS,
    flow_iterations=constants.FLOW_ITERATIONS,
    duration=constants.DURATION,
    flow_duration=constants.FLOW_DURATION,
    poll=constants.POLL,
    random_read=constants.RANDOM_READ,
    # files
    data_filepath=constants.DATA_FILEPATH,
    search_optimal_filepath=constants.SEARCH_OPTIMAL_FILEPATH,
    summary_filepath=constants.SUMMARY_FILEPATH,
    smart_filepath=constants.SMART_FILEPATH,
    # lists
    steps=None,
    byte_array=None,
    ignore_partitions=None,
    include_partitions=None,
    disk_numbers=None,
    # dicts
    disk_number_to_letter_dict=None,
    # bools
    search_optimal=constants.SEARCH_OPTIMAL,
    no_optimizations=False,
    burn_in=constants.BURN_IN,
    no_crystaldiskinfo=constants.NO_CRYSTALDISKINFO,
    all_drives=constants.ALL_DRIVES,
    no_telemetry=constants.NO_TELEMETRY,
    no_admin=constants.NO_ADMIN,
    delete=constants.DELETE,
):
    if func not in FUNCS:
        raise KeyError(f'func {func!r} does not exist, use one of {FUNCS}!')
    if log_level not in constants.LOG_LEVELS:
        raise KeyError(f'log_level {log_level!r} does not exist, choose one of {constants.LOG_LEVELS}!')
    if log_unit not in constants.LOG_UNITS:
        raise KeyError(f'log_unit {log_unit!r} does not exist, choose one of {constants.LOG_UNITS}!')

    # numbers
    if log_mod != constants.LOG_MOD and log_mod <= 0:
        raise ValueError('log_mod must be a postive int, are you nuts?')
    if value != -1:
        if value < 0 and 255 < value:
            raise ValueError('value must be a value between [0,255] or -1')
    if isinstance(size, str):
        try:
            size = int(size)
        except ValueError:
            size = lib.size_unit_convert(size)
        args.size = size
    if size != constants.SIZE and size <= 0:
        raise ValueError('size must be a postive int, are you nuts?')
    if iterations < constants.ITERATIONS:
        raise ValueError('iterations must be a postive num (or -1), are you nuts?')
    if flow_iterations != -1 and flow_iterations <= 0:
        raise ValueError('flow_iterations must be a postive num (or -1), are you nuts?')
    if duration != constants.DURATION and duration <= 0:
        raise ValueError('duration must be a postive num (or -1), are you nuts?')
    if flow_duration != constants.FLOW_DURATION and flow_duration <= 0:
        raise ValueError('flow_duration must be a postive num (or -1), are you nuts?')
    if poll < 0:
        raise ValueError('poll must be positive!')
    if isinstance(random_read, str):
        try:
            random_read = int(random_read)
        except ValueError:
            random_read = lib.size_unit_convert(random_read)
        args.random_read = random_read
    if random_read != constants.RANDOM_READ and random_read <= 0:
        raise ValueError('random_read must be positive!')

    # files
    for filepath in [data_filepath, search_optimal_filepath, summary_filepath, smart_filepath]:
        if not os.path.isdir(os.path.dirname(filepath)):
            os.makedirs(os.path.dirname(filepath), exist_ok=True)

    # lists
    steps = steps or []
    if not isinstance(steps, list):
        raise TypeError(f'steps must be of type bool, provided {type(steps)}')
    assert all(step in FUNC_NAMES for step in steps), f'not all steps provided are real, only these are: {FUNC_NAMES}'
    byte_array = byte_array or bytearray()
    if not isinstance(byte_array, bytearray):
        raise TypeError(f'byte_array must be of type bytearray, provided {type(byte_array)}')
    if isinstance(ignore_partitions, list):
        for i, ignore_partition in enumerate(ignore_partitions):
            if ignore_partition not in string.ascii_uppercase:
                raise ValueError(f'ignore_partition {i + 1} {ignore_partition!r} not in expected possibilities!')
    if isinstance(include_partitions, list):
        for i, include_partition in enumerate(include_partitions):
            if include_partition not in string.ascii_uppercase:
                raise ValueError(f'include_partition {i + 1} {include_partition!r} not in expected possibilities!')
    if isinstance(disk_numbers, list):
        for i, disk_number in enumerate(disk_numbers):
            fail = False
            try:
                int(disk_number)
            except ValueError:
                fail = True
            if fail:
                raise ValueError(f'disk_number {i + 1} is not an int!')

    # dicts
    if disk_number_to_letter_dict:
        disk_number_to_letter_dict = json.loads(disk_number_to_letter_dict)
        args.disk_number_to_letter_dict = disk_number_to_letter_dict

    # bools
    if not isinstance(no_optimizations, bool):
        raise TypeError(f'no_optimizations must be of type bool, provided {type(no_optimizations)}')
    if not isinstance(search_optimal, bool):
        raise TypeError(f'search_optimal must be of type bool, provided {type(search_optimal)}')
    if not isinstance(burn_in, bool):
        raise TypeError(f'burn_in must be of type bool, provided {type(burn_in)}')
    if not isinstance(no_crystaldiskinfo, bool):
        raise TypeError(f'no_crystaldiskinfo must be of type bool, provided {type(no_crystaldiskinfo)}')
    if not isinstance(all_drives, bool):
        raise TypeError(f'all_drives must be of type bool, provided {type(all_drives)}')
    if not isinstance(no_telemetry, bool):
        raise TypeError(f'no_telemetry must be of type bool, provided {type(no_telemetry)}')
    if not isinstance(no_admin, bool):
        raise TypeError(f'no_admin must be of type bool, provided {type(no_admin)}')
    if not isinstance(delete, bool):
        raise TypeError(f'delete must be of type bool, provided {type(delete)}')


def main():
    parser = argparse.ArgumentParser(prog=constants.APP_NAME, description=__doc__, formatter_class=NiceFormatter)
    operations = parser.add_subparsers(help='different operations we can do')
    for func in FUNCS:
        op = operations.add_parser(
            func.__name__,
            help=func.__doc__.strip().splitlines()[1].strip(),  # just under Description:
            description=func.__doc__,
            formatter_class=NiceFormatter,
        )
        op.set_defaults(func=func)

        if func is flow:
            # this one is especially meta because it "consumes" all kwargs, so we have to give it ALL arguments
            for group_name, argdict in ARGUMENTS.items():
                group = op.add_argument_group(group_name)
                for key, argparse_kwargs in argdict.items():
                    names = []
                    if '_' in key:
                        names.append(f'--{key.replace("_", "-")}')
                        names.append(f'--{key}')
                    else:
                        names.append(f'--{key}')
                    try:
                        if argparse_kwargs['type'] is bool:
                            group.add_argument(
                                *names,
                                action='store_true',
                                **{
                                    key: value
                                    for key, value in argparse_kwargs.items() if key != 'type'
                                }
                            )
                        else:
                            group.add_argument(*names, **argparse_kwargs)
                    except argparse.ArgumentError as ae:
                        aestr = str(ae)
                        if 'conflicting option strings' in aestr:
                            pass

        else:
            # all other functions are "normal" and follow other similar rules
            group = op.add_argument_group('operation')
            signature = inspect.signature(func)
            for key in signature.parameters:
                # print(func, key)
                argparse_kwargs = {k: v for k, v in OPERATION_ARGUMENTS.get(key, {}).items()}  # copy
                if 'default' in argparse_kwargs:
                    default = argparse_kwargs['default']
                    func_default = signature.parameters[key].default
                    if default != func_default:
                        argparse_kwargs['default'] = func_default
                if not argparse_kwargs:
                    continue
                names = []
                if '_' in key:
                    names.append(f'--{key.replace("_", "-")}')
                    names.append(f'--{key}')
                else:
                    names.append(f'--{key}')
                if argparse_kwargs['type'] is bool:
                    group.add_argument(
                        *names,
                        action='store_true',
                        **{
                            key: value
                            for key, value in argparse_kwargs.items() if key != 'type'
                        }
                    )
                else:
                    group.add_argument(*names, **argparse_kwargs)

            group = op.add_argument_group('telemetry')
            signature = inspect.signature(telemetry)
            for key, argparse_kwargs in TELEMETRY_ARGUMENTS.items():
                argparse_kwargs = copy.deepcopy(argparse_kwargs)
                # see if the function defines a special value instead, and use that.
                if 'default' in argparse_kwargs and key in signature.parameters:
                    default = argparse_kwargs['default']
                    func_default = signature.parameters[key].default
                    if default != func_default:
                        argparse_kwargs['default'] = func_default
                names = []
                if '_' in key:
                    names.append(f'--{key.replace("_", "-")}')
                    names.append(f'--{key}')
                else:
                    names.append(f'--{key}')
                try:
                    if argparse_kwargs['type'] is bool:
                        group.add_argument(
                            *names,
                            action='store_true',
                            **{
                                key: value
                                for key, value in argparse_kwargs.items() if key != 'type'
                            }
                        )
                    else:
                        group.add_argument(*names, **argparse_kwargs)
                except argparse.ArgumentError as ae:
                    aestr = str(ae)
                    if 'conflicting option strings' in aestr:
                        pass

            # for key in signature.parameters:
            #     # print(func, key)
            #     # if key in OPERATION_ARGUMENTS:
            #     #     continue
            #     if func == write:
            #         print('plx')
            #     if key not in TELEMETRY_ARGUMENTS:
            #         continue
            #     argparse_kwargs = {k: v for k, v in TELEMETRY_ARGUMENTS.get(key, {}).items()}  # copy
            #     if 'default' in argparse_kwargs:
            #         default = argparse_kwargs['default']
            #         func_default = signature.parameters[key].default
            #         if default != func_default:
            #             argparse_kwargs['default'] = func_default
            #     if not argparse_kwargs:
            #         continue
            #     names = []
            #     if '_' in key:
            #         names.append(f'--{key.replace("_", "-")}')
            #         names.append(f'--{key}')
            #     else:
            #         names.append(f'--{key}')
            #     try:
            #         if argparse_kwargs['type'] is bool:
            #             group.add_argument(
            #                 *names,
            #                 action='store_true',
            #                 **{
            #                     key: value
            #                     for key, value in argparse_kwargs.items() if key != 'type'
            #                 }
            #             )
            #         else:
            #             group.add_argument(*names, **argparse_kwargs)
            #     except argparse.ArgumentError as ae:
            #         aestr = str(ae)
            #         if 'conflicting option strings' in aestr:
            #             pass

    args = parser.parse_args()
    func = args.func

    kwargs = vars(args)
    validate_kwargs(args, **kwargs)  # update by side effect
    logging.basicConfig(
        format='%(asctime)s - %(levelname)10s - %(funcName)48s - %(message)s',
        level=args.log_level,
        stream=sys.stdout,
        force=True
    )
    kwargs = vars(args)
    logging.debug(pprint.pformat(kwargs, indent=2))

    telemetry_signature = inspect.signature(telemetry_thread)
    telemetry_thread_kwargs = {k: kwargs[k] for k in telemetry_signature.parameters if k in TELEMETRY_ARGUMENTS}
    telemetry_thread_kwargs.update({k: kwargs[k] for k in telemetry_signature.parameters if k in OPERATION_ARGUMENTS})
    logging.debug('telemetry_thread_kwargs: %s', pprint.pformat(telemetry_thread_kwargs, indent=2))

    func_signature = inspect.signature(func)
    func_kwargs = {k: kwargs[k] for k in func_signature.parameters if k in OPERATION_ARGUMENTS}
    func_kwargs.update({k: kwargs[k] for k in func_signature.parameters if k in TELEMETRY_ARGUMENTS})
    logging.debug('func_kwargs: %s', pprint.pformat(func_kwargs, indent=2))

    logging.info('starting %r', func.__name__)
    stop_event = threading.Event()
    success = True
    stop_event, thread = None, None
    try:
        if not any(
            [
                func is flow and 'telemetry' in args.steps,
                func in {telemetry, create_partitions, delete_partitions},
            ]
        ):
            stop_event, thread = telemetry_thread(**telemetry_thread_kwargs)

        logging.info('starting %r', func.__name__)
        func(**func_kwargs)
    except KeyboardInterrupt:
        logging.warning('ctrl + c detected!')
    except Exception as ex:
        logging.debug('ERROR: exception encountered during execution, enable --log-level DEBUG!', exc_info=True)
        logging.error(str(ex))
        success = False
    finally:
        if stop_event:
            stop_event.set()
            thread.join()
        if success:
            logging.info('success!')
        else:
            logging.error('failure!')


if __name__ == '__main__':
    main()
