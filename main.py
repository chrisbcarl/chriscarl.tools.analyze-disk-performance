# encoding: utf-8
'''
The MIT License (MIT)
Copyright © 2023 Chris Carl <chrisbcarl@outlook.com>
Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the “Software”), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all
  copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.

Author:     Chris Carl <chrisbcarl@outlook.com>
Date:       2024-09-26
Modified:   2024-09-26

Modified:
    2025-08-05 - chrisbcarl - moved files to the root of the perf filepath so the files
                                dont contend with each other for bandwidth or space
    2025-08-03 - chrisbcarl - moved functions around, perfected the read_bytearray_from_disk
                              cleaned up perf+fill+read minutia
    2025-08-02 - chrisbcarl - added perf+fill+read and smartmon
    2025-08-01 - chrisbcarl - changed to chriscarl.tools.analyze-disk-performance
                              created the 'health' module which is designed to rip through all disks
                              added crystaldiskinfo utilization, we'll see how it goes after testing.
    2024-09-28 - chrisbcarl - added "write" mode, much easier.
    2024-09-26 - chrisbcarl - complete rewrite with different modes this time
    2023-07-05 - chrisbcarl - init commit, behaves like sequential write/readback, thats it.

TODO:
The overall flow should be something like this...
python main.py departition partition create write read departition --size 100mb --value 69 --loop 5 (implying that the interior code is)
    for i in 5:
        for operation in create, write, read:
            execute
smart is always running in the background

operations:
    departition
    partition
    create (generic random of some size)
    create-fast  (finds the fastest size)
    write
    read
    read-random (create a bunch of random windows but the data is whatever the data was)

files to keep track of
    smart performance csv
    create-fast performance csv
    summary csv


    - move toward list operations rather than explicitly perf + write, like you can mix perf write loop etc, implying
        the bytearray generated by perf is used by write and loop
    - deal with the situation where create_bytearray would result in a bytearray the size larger than the universe.
        better would be to make some object that when you ask for an index or the next byte,
        it GENRATES it, is writable, etc...
    - dynamic crystaldiskinfo txt

Examples:
    - What most people think of as write-read benchmarking
        python main.py perf+fill+read --data-filepath Y:/temp
    - Find performance sweetspot and fill the disk at partition D:/
        python main.py perf+fill --data-filepath D:/temp
    - Evaluate overall health on all newly inserted disks
        python main.py health --ignore-partitions C --log-level DEBUG
    - Just launch a cyrstaldiskinfo monitor
        python main.py smartmon
'''
# stdlib
from __future__ import print_function, division
import os
import sys
import re
import csv
import time
import copy
import json
import shutil
import string
import random
import logging
import argparse
import datetime
import threading
import subprocess
import multiprocessing
from typing import Tuple, Dict

# 3rd party
import pandas as pd
import psutil

# local
import constants
from constants import (
    DATA_FILEPATH,
    PERF_FILEPATH,
    SEARCH_OPTIMAL_FILEPATH,
    SUMMARY_FILEPATH,
    VALUE,
    DURATION,
    ITERATIONS,
)
from lib import (
    abspath,
    get_keys_from_dicts,
    create_bytearray,
    create_bytearray_killobytes,
    disk_usage_monitor,
    write_burnin,
    write_fulpak,
    create_byte_array_high_throughput,
    write_bytearray_to_disk,
    read_bytearray_from_disk,
    generate_and_write_bytearray,
    crystaldiskinfo_parse,
    crystaldiskinfo,
)

SCRIPT_DIRPATH = os.path.abspath(os.path.dirname(__file__))
OPERATIONS = ['perf', 'fill', 'perf+fill', 'loop', 'write', 'perf+write', 'health', 'perf+fill+read', 'smartmon']
LOG_LEVELS = list(logging._nameToLevel)  # pylint: disable=(protected-access)
LOG_LEVEL = 'INFO'

# arg defaults
CPU_COUNT = multiprocessing.cpu_count()
SIZE = 1
POLL = 15
LOOPS = -1


class NiceFormatter(
    argparse.ArgumentDefaultsHelpFormatter, argparse.RawTextHelpFormatter, argparse.RawDescriptionHelpFormatter
):
    pass


def validate_kwargs(
    operation=OPERATIONS[0],
    log_level='INFO',
    value=VALUE,
    size=SIZE,
    duration=DURATION,
    iterations=ITERATIONS,
    no_optimizations=False,
    data_filepath=DATA_FILEPATH,
    perf_filepath=PERF_FILEPATH,
    search_optimal_filepath=SEARCH_OPTIMAL_FILEPATH,
    summary_filepath=SUMMARY_FILEPATH,
    ignore_partitions=None,
    poll=POLL,
):
    if operation not in OPERATIONS:
        raise KeyError(f'operation {operation!r} does not exist, use one of {OPERATIONS}!')
    if log_level not in LOG_LEVELS:
        raise KeyError(f'log_level {log_level!r} does not exist!')
    if value != -1:
        if value < 0 and 255 < value:
            raise ValueError('value must be a value between [0,255] or -1')
    if size < 1:
        raise ValueError('size must be a postive int, are you nuts?')
    if duration < 0 and duration != -1:
        raise ValueError('duration must be a postive num (or -1), are you nuts?')
    if iterations < -1:
        raise ValueError('iterations must be a postive num (or -1), are you nuts?')
    if not isinstance(no_optimizations, bool):
        raise TypeError(f'no_optimizations must be of type bool, provided {type(no_optimizations)}')
    for filepath in [data_filepath, perf_filepath, search_optimal_filepath, summary_filepath]:
        if not os.path.isdir(os.path.dirname(filepath)):
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
    if isinstance(ignore_partitions, list):
        for ignore_partition in ignore_partitions:
            if ignore_partition not in string.ascii_uppercase:
                raise ValueError(f'ignore_partition {ignore_partition!r} not in expected possibilities!')
    if poll < 0:
        raise ValueError('poll must be positive!')


def perf_fill_read(
    data_filepath=DATA_FILEPATH,
    perf_filepath=PERF_FILEPATH,
    search_optimal_filepath=SEARCH_OPTIMAL_FILEPATH,
    value=VALUE,
    iterations=ITERATIONS,
    duration=-1
):
    sweetspot_byte_array = create_byte_array_high_throughput(
        data_filepath=data_filepath, search_optimal_filepath=search_optimal_filepath, value=value
    )
    logging.info(
        'data_filepath="%s", perf_filepath="%s", search_optimal_filepath="%s", value=%s, iterations=%s, duration=%s',
        data_filepath, perf_filepath, search_optimal_filepath, value, iterations, duration
    )
    iteration = 1
    if iterations > -1:
        for iteration in range(1, iterations + 1):
            try:
                logging.info('fill+read iteration %d / %d', iteration, iterations)
                write_fulpak(sweetspot_byte_array, data_filepath=data_filepath)
                logging.info('read iteration %d / %d', iteration, iterations)
                read_bytearray_from_disk(sweetspot_byte_array, data_filepath=data_filepath)
            except KeyboardInterrupt:
                logging.warning('ctrl + c detected!')
                break
    elif duration > -1:
        start = time.time()
        elapsed = time.time() - start
        while elapsed < duration:
            try:
                logging.info('fill+read iteration %d until %s > %s', iteration, elapsed, duration)
                write_fulpak(sweetspot_byte_array, data_filepath=data_filepath)
                if elapsed > duration:
                    logging.info('time exceeded after write')
                    break
                logging.info('read iteration %d / %d', iteration, iterations)
                read_bytearray_from_disk(sweetspot_byte_array, data_filepath=data_filepath)
                if elapsed > duration:
                    logging.info('time exceeded after read')
                    break

                iteration += 1
                elapsed = time.time() - start
            except KeyboardInterrupt:
                logging.warning('ctrl + c detected!')
                break
    else:
        while True:
            try:
                logging.info('fill+read iteration %d infinitely', iteration)
                write_fulpak(sweetspot_byte_array, data_filepath=data_filepath)
                logging.info('read iteration %d / %d', iteration, iterations)
                read_bytearray_from_disk(sweetspot_byte_array, data_filepath=data_filepath)

                iteration += 1
            except KeyboardInterrupt:
                logging.warning('ctrl + c detected!')
                break


def add_options(parser, operation, *options):
    parser.set_defaults(operation=operation)
    group = parser.add_argument_group(f'{operation!r} specific')
    for option in options:
        if option == '--size' or option == "*":
            group.add_argument('--size', type=int, default=SIZE, help='size in killobytes, so --size * 1024B')
        elif option == '--duration' or option == "*":
            group.add_argument(
                '--duration',
                type=int,
                default=DURATION,
                help='either run till --duration in seconds, -1 for infinitely'
            )
        elif option == '--iterations' or option == "*":
            group.add_argument(
                '--iterations',
                type=int,
                default=ITERATIONS,
                help='default infinitely, the amount of discrete repetitions'
            )
        elif option == '--no-optimizations' or option == "*":
            group.add_argument(
                '--no-optimizations',
                action='store_true',
                help='generate the FULL byte_array in memory, no matter how unreasonable.'
            )
        elif option == '--ignore-partitions' or option == "*":
            group.add_argument(
                '--ignore-partitions',
                type=str,
                nargs='*',
                default=['C', 'D', 'E'],
                help='if known partitions, ignore these'
            )
        elif option == '--poll' or option == "*":
            group.add_argument('--poll', type=float, default=POLL, help='telemetry poll poll')
        elif option == '--operation' or option == "*":
            group.add_argument(
                '--operation',
                type=str,
                default='perf+fill+read',
                choices=OPERATIONS,
                help='which operation should I spread onto all disks?'
            )

    group = parser.add_argument_group('general')
    group.add_argument(
        '--data-filepath', type=str, default=DATA_FILEPATH, help='where to dump the file that fills the disk.'
    )
    group.add_argument(
        '--perf-filepath', type=str, default=PERF_FILEPATH, help='where to dump the csv with performance data.'
    )
    group.add_argument(
        '--byte-array-throughput-filepath',
        type=str,
        default=SEARCH_OPTIMAL_FILEPATH,
        help='where to dump the csv with byte array throughput data.'
    )
    group.add_argument('--summary-filepath', type=str, default=SUMMARY_FILEPATH, help='afteraction summary')
    group.add_argument(
        '--value', type=int, default=VALUE, help='fill bytearray with a constant byte value, default means random.'
    )
    group.add_argument('--log-level', type=str, default=LOG_LEVEL, choices=LOG_LEVELS, help='log level')


def main():
    parser = argparse.ArgumentParser(prog='fill-the-drive', description=__doc__, formatter_class=NiceFormatter)
    operations = parser.add_subparsers(help='different operations we can do')

    add_options(
        operations.add_parser(
            'perf',
            help='analyze the performance of the drive which determines a file size that is fastest to write',
            description=create_byte_array_high_throughput.__doc__,
            formatter_class=NiceFormatter,
        ),
        'perf',
    )
    add_options(
        operations.add_parser(
            'fill',
            help='fill up the disk',
            description=write_fulpak.__doc__,
            formatter_class=NiceFormatter,
        ), 'fill', '--size'
    )
    add_options(
        operations.add_parser(
            'perf+fill',
            help='do perf+fill',
            description=write_fulpak.__doc__,
            formatter_class=NiceFormatter,
        ),
        'perf+fill',
    )
    add_options(
        operations.add_parser(
            'loop',
            help='repeatedly write to the disk for some size and duration',
            description=write_burnin.__doc__,
            formatter_class=NiceFormatter,
        ), 'loop', '--size', '--duration', '--iterations'
    )
    add_options(
        operations.add_parser(
            'write',
            help='make a new file with a particular size',
            description=generate_and_write_bytearray.__doc__,
            formatter_class=NiceFormatter,
        ), 'write', '--size', '--no-optimizations'
    )
    add_options(
        operations.add_parser(
            'perf+write',
            help='make a new file with a particular size',
            description=f'{create_byte_array_high_throughput.__doc__}\n{write_bytearray_to_disk.__doc__}',
            formatter_class=NiceFormatter,
        ), 'perf+write', '--size', '--no-optimizations'
    )
    add_options(
        operations.add_parser(
            'perf+fill+read',
            help='fill disk, readback',
            description=perf_fill_read.__doc__,
            formatter_class=NiceFormatter,
        ), 'perf+fill+read', '--iterations', '--duration'
    )
    add_options(
        operations.add_parser(
            'health',
            help='evaluate all disks for their health and track the movements',
            description=generate_and_write_bytearray.__doc__,
            formatter_class=NiceFormatter,
        ), 'health', '--ignore-partitions', '--poll', '--iterations', '--duration', '--operation'
    )
    add_options(
        operations.add_parser(
            'smartmon',
            help='repeatedly run crystaldiskinfo',
            description='TODO: plz',  # TODO: plz
            formatter_class=NiceFormatter,
        ),
        'smartmon',
        '--poll'
    )
    args = parser.parse_args()
    validate_kwargs(**vars(args))

    logging.basicConfig(
        format='%(asctime)s - %(levelname)10s - %(funcName)48s - %(message)s', level=args.log_level, stream=sys.stdout
    )
    logging.info('starting %r', args.operation)

    if args.operation == 'perf':
        create_byte_array_high_throughput(
            value=args.value, data_filepath=args.data_filepath, search_optimal_filepath=args.search_optimal_filepath
        )

    elif args.operation == 'perf+fill':
        sweetspot_byte_array = create_byte_array_high_throughput(
            data_filepath=args.data_filepath, search_optimal_filepath=args.search_optimal_filepath, value=args.value
        )
        write_fulpak(sweetspot_byte_array, data_filepath=args.data_filepath)

    elif args.operation in ['fill', 'loop']:
        byte_array = create_bytearray_killobytes(args.size, value=args.value)
        if args.operation == 'fill':
            write_fulpak(byte_array, data_filepath=args.data_filepath)
        elif args.operation == 'loop':
            write_burnin(
                byte_array, data_filepath=args.data_filepath, duration=args.duration, iterations=args.iterations
            )

    elif args.operation == 'write':
        generate_and_write_bytearray(
            args.size,
            value=args.value,
            no_optimizations=args.no_optimizations,
            data_filepath=args.data_filepath,
            randomness=False
        )

    elif args.operation == 'perf+write':
        sweetspot_byte_array = create_byte_array_high_throughput(
            data_filepath=args.data_filepath, search_optimal_filepath=args.search_optimal_filepath, value=args.value
        )
        write_bytearray_to_disk(sweetspot_byte_array, size=args.size, data_filepath=args.data_filepath)

    elif args.operation == 'perf+fill+read':
        perf_fill_read(
            data_filepath=args.data_filepath,
            perf_filepath=args.perf_filepath,
            search_optimal_filepath=args.search_optimal_filepath,
            summary_filepath=args.summary_filepath,
            value=args.value,
            iterations=args.iterations,
            duration=args.duration
        )

    elif args.operation == 'smartmon':
        logging.debug('reading crystaldiskinfo')
        crystal_data = crystaldiskinfo()
        crystaldisk_keys = get_keys_from_dicts(*list(crystal_data.values()))
        if not os.path.isfile(args.perf_filepath):
            with open(args.perf_filepath, 'w', encoding='utf-8', newline='') as w:
                writer = csv.DictWriter(w, fieldnames=crystaldisk_keys)
                writer.writeheader()
                for value in crystal_data.values():
                    writer.writerow(value)
        else:
            # combine old and new keys
            old_df = pd.from_csv(args.perf_filepath)
            new_df = pd.DataFrame(crystal_data.values())
            df = pd.concat([old_df, new_df])
            df.to_csv(index=False)
            crystaldisk_keys = df.columns.tolist()

        try:
            started = datetime.datetime.now()
            while True:
                now = datetime.datetime.now()
                logging.info('elapsed: %s', now - started)
                crystal_data = crystaldiskinfo()
                with open(args.perf_filepath, 'a', encoding='utf-8', newline='') as a:
                    writer = csv.DictWriter(a, fieldnames=crystaldisk_keys)
                    for value in crystal_data.values():
                        writer.writerow(value)
                time.sleep(args.poll)
        except KeyboardInterrupt:
            logging.warning('ctrl + c detected! killing processes, removing resources...')

    elif args.operation == 'health':
        try:
            output = subprocess.check_output(
                ['where.exe' if sys.platform == 'win32' else 'which', constants.CRYSTALDISKINFO_EXE],
                universal_newlines=True
            )
            for line in output.splitlines():
                strip = line.strip()
                if os.path.isfile(strip):
                    constants.CRYSTALDISKINFO_EXE = strip
                    break
            if not os.path.isfile(strip):
                raise OSError(f'Could not find "{constants.CRYSTALDISKINFO_EXE}"!')
        except subprocess.CalledProcessError:
            logging.warning('WARNING: CrystalDiskInfo not installed or not on path!')
            return 1

        if sys.platform != 'win32':
            raise NotImplementedError(sys.platform)

        logging.debug('checking admin access')
        admin_ps1 = abspath(SCRIPT_DIRPATH, r"scripts\win32\admin.ps1")
        subprocess.check_call(['powershell', admin_ps1])
        logging.info('admin detected!')

        # get current partitions
        logging.debug('sanitizing unwanted partitions')
        read_partitions_ps1 = abspath(SCRIPT_DIRPATH, r"scripts\win32\read-partitions.ps1")
        cmd = ['powershell', read_partitions_ps1]
        logging.debug(subprocess.list2cmdline(cmd))
        output = subprocess.check_output(cmd, universal_newlines=True)
        read_partitions = json.loads(output)
        logging.debug('partitions: %s', json.dumps(read_partitions, indent=2))
        logging.info('read partitions!')

        logging.debug('filter out all partitions that dont belong')
        drive_letters_to_remove = [key for key in read_partitions if key not in args.ignore_partitions]
        logging.debug('removing drive letters: %s', drive_letters_to_remove)
        disk_numbers = [val['DiskNumber'] for key, val in read_partitions.items() if key not in args.ignore_partitions]
        logging.debug('will operate on drive numbers: %s', disk_numbers)
        logging.info('found disks and old partitions!')

        if drive_letters_to_remove:
            logging.debug('remove partitions so they return to raw')
            delete_partitions_ps1 = abspath(SCRIPT_DIRPATH, r"scripts\win32\delete-partitions.ps1")
            cmd = ['powershell', delete_partitions_ps1, '-DriveLetters', ','.join(drive_letters_to_remove)]
            logging.debug(subprocess.list2cmdline(cmd))
            output = subprocess.check_output(cmd, universal_newlines=True)
            logging.debug(output)
            logging.info('removed unwanted partitions!')

        logging.debug('make new partitions')
        create_partitions_ps1 = abspath(SCRIPT_DIRPATH, r"scripts\win32\create-partitions.ps1")
        cmd = ['powershell', create_partitions_ps1, '-DriveLetters', ','.join(drive_letters_to_remove)]
        logging.debug(subprocess.list2cmdline(cmd))
        output = subprocess.check_output(cmd, universal_newlines=True)
        create_partitions_sentinel = "Begin Output Parsing Here:"
        logging.debug(output)
        output = output[output.find(create_partitions_sentinel) + len(create_partitions_sentinel) + 1:].strip()
        logging.debug(output)
        drive_number_to_letter_dict = json.loads(output)
        logging.debug('will operate on drive numbers and letters: %s', drive_number_to_letter_dict)
        logging.info('created new partitions!')

        logging.debug('reading disks')
        read_disks_ps1 = abspath(SCRIPT_DIRPATH, r"scripts\win32\read-disks.ps1")
        cmd = ['powershell', read_disks_ps1]
        logging.debug(subprocess.list2cmdline(cmd))
        output = subprocess.check_output(cmd, universal_newlines=True)
        read_disks = json.loads(output)
        logging.debug('disks: %s', json.dumps(read_disks, indent=2))
        logging.debug('read disk info!')

        logging.debug('reading crystaldiskinfo')
        crystal_data = crystaldiskinfo()
        crystaldisk_keys = get_keys_from_dicts(*list(crystal_data.values()))
        if not os.path.isfile(args.perf_filepath):
            with open(args.perf_filepath, 'w', encoding='utf-8', newline='') as w:
                writer = csv.DictWriter(w, fieldnames=crystaldisk_keys)
                writer.writeheader()
        with open(args.perf_filepath, 'a', encoding='utf-8', newline='') as a:
            writer = csv.DictWriter(a, fieldnames=crystaldisk_keys)
            for value in crystal_data.values():
                writer.writerow(value)
        logging.info('read crystaldiskinfo!')

        popens = []
        operation = args.operation
        logging.info('starting %r', operation)
        started = datetime.datetime.now()
        output_dirpath = os.path.dirname(args.perf_filepath)
        for drive_number, drive_letter in drive_number_to_letter_dict.items():
            data_filepath = abspath(f'{drive_letter}:/{drive_number}-{operation}.dat')
            perf_filepath = abspath(f'{output_dirpath}/{drive_number}-{operation}.csv')
            search_optimal_filepath = abspath(f'{output_dirpath}/{drive_number}-{operation}-bytearray.csv')
            stdout = abspath(f'{output_dirpath}/{drive_number}-{operation}.stdout')
            if operation == 'perf+fill+read':
                cmd = [
                    sys.executable,
                    abspath(__file__),
                    operation,
                    '--data-filepath',
                    data_filepath,
                    '--perf-filepath',
                    perf_filepath,
                    '--byte-array-throughput-filepath',
                    search_optimal_filepath,
                    '--value',
                    str(args.value),
                    '--log-level',
                    str(args.log_level),
                    '--iterations',
                    str(args.iterations),
                    '--duration',
                    str(args.duration),
                ]
            else:
                raise NotImplementedError(
                    f'underlying subprocess commandline not accounted for as part of operation {operation!r}'
                )
            logging.debug('drive %s (%s): %s', drive_number, drive_letter, subprocess.list2cmdline(cmd))
            with open(stdout, 'wb') as sout:
                popen = subprocess.Popen(cmd, stdout=sout)
                popens.append(popen)

        logging.info('launching multi %r on %d partitions/drives...', operation, len(drive_number_to_letter_dict))
        try:
            while True:
                now = datetime.datetime.now()
                logging.info('elapsed: %s', now - started)
                crystal_data = crystaldiskinfo()
                with open(args.perf_filepath, 'a', encoding='utf-8', newline='') as a:
                    writer = csv.DictWriter(a, fieldnames=crystaldisk_keys)
                    for value in crystal_data.values():
                        writer.writerow(value)
                if all([popen.poll() is not None for popen in popens]):
                    logging.info('All %r finished!', operation)
                    break
                time.sleep(args.poll)
        except KeyboardInterrupt:
            logging.warning('ctrl + c detected! killing processes, removing resources...')

        logging.info('closing resources...')
        for popen in popens:
            if popen.poll() is None:
                popen.kill()
                subprocess.Popen(['taskkill', '/pid', str(popen.pid), '/f', '/t'], shell=True).wait()
        for drive_number, drive_letter in drive_number_to_letter_dict.items():
            data_filepath = abspath(f'{drive_letter}:/{drive_number}-{mode}.dat')
            try:
                os.remove(data_filepath)
            except Exception:
                logging.error('unable to delete ""%s', data_filepath)

        logging.debug('removing partitions...')
        delete_partitions_ps1 = abspath(SCRIPT_DIRPATH, r"scripts\win32\delete-partitions.ps1")
        cmd = [
            'powershell', delete_partitions_ps1, '-Offline', '-DriveLetters',
            ','.join(drive_number_to_letter_dict.values())
        ]
        logging.debug(subprocess.list2cmdline(cmd))
        output = subprocess.check_output(cmd, universal_newlines=True)
        logging.debug(output)
        logging.info('removed all used partitions!')

        crystal_keys = [
            'Health Status', 'Disk Number', 'Model', 'Serial Number', 'Disk Size', 'Power On Count', 'Host Reads',
            'Host Writes', 'End to End Error Detection Count', 'Uncorrectable Error Count',
            'Media and Data Integrity Errors', 'Number of Error Information Log Entries'
        ]
        cdi_df = pd.load_csv(args.perf_filepath)
        summary_df = cdi_df[crystal_keys][pd.notna(cdi_df['Disk Number'])]
        summary_df.to_csv(args.summary_filepath, index=False)

    logging.info('perf-filepath: "%s"', args.perf_filepath)
    logging.info('done %r', args.operation)


if __name__ == '__main__':
    main()
