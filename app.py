# encoding: utf-8
'''
The MIT License (MIT)
Copyright © 2023 Chris Carl <chrisbcarl@outlook.com>
Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the “Software”), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all
  copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.

Author:     Chris Carl <chrisbcarl@outlook.com>
Date:       2024-09-26
Modified:   2024-09-26

Modified:
    2025-08-07 - chrisbcarl - nearly full re-write, re-orged into app.py, will be further developed on
    2025-08-05 - chrisbcarl - moved files to the root of the perf filepath so the files
                                dont contend with each other for bandwidth or space
    2025-08-03 - chrisbcarl - moved functions around, perfected the read_bytearray_from_disk
                              cleaned up perf+fill+read minutia
    2025-08-02 - chrisbcarl - added perf+fill+read and smartmon
    2025-08-01 - chrisbcarl - changed to chriscarl.tools.analyze-disk-performance
                              created the 'health' module which is designed to rip through all disks
                              added crystaldiskinfo utilization, we'll see how it goes after testing.
    2024-09-28 - chrisbcarl - added "write" mode, much easier.
    2024-09-26 - chrisbcarl - complete rewrite with different modes this time
    2023-07-05 - chrisbcarl - init commit, behaves like sequential write/readback, thats it.

TODO:
The overall flow should be something like this...
python main.py departition partition create write read departition --size 100mb --value 69 --loop 5 (implying that the interior code is)
    for i in 5:
        for operation in create, write, read:
            execute
smart is always running in the background

operations:
    departition
    partition
    create (generic random of some size)
    create-fast  (finds the fastest size)
    write
    read
    read-random (create a bunch of random windows but the data is whatever the data was)

files to keep track of
    smart performance csv
    create-fast performance csv
    summary csv


    - move toward list operations rather than explicitly perf + write, like you can mix perf write loop etc, implying
        the bytearray generated by perf is used by write and loop
    - deal with the situation where create_bytearray would result in a bytearray the size larger than the universe.
        better would be to make some object that when you ask for an index or the next byte,
        it GENRATES it, is writable, etc...
    - dynamic crystaldiskinfo txt

Examples:
    - What most people think of as write-read benchmarking
        python main.py perf+fill+read --data-filepath Y:/temp
    - Find performance sweetspot and fill the disk at partition D:/
        python main.py perf+fill --data-filepath D:/temp
    - Evaluate overall health on all newly inserted disks
        python main.py health --ignore-partitions C --log-level DEBUG
    - Just launch a cyrstaldiskinfo monitor
        python main.py smartmon
    - Write a 1kb file of 69s 10 times
        python app.py write --size 1024 --value 69 --data-filepath Y:/tmp --iterations 10
'''
# stdlib
from __future__ import print_function, division
import os
import sys
import re
import csv
import time
import copy
import json
import pprint
import string
import random
import logging
import inspect
import argparse
import datetime
import threading
import subprocess
import multiprocessing
from typing import Tuple, Dict, List, Optional

# 3rd party
import pandas as pd
import psutil

# app imports
import lib
import constants
from stdlib import NiceFormatter, abspath

SCRIPT_DIRPATH = os.path.abspath(os.path.dirname(__file__))


def _telemetry_smart(event):
    # type: (threading.Event) -> Tuple[pd.DataFrame, dict]
    '''
    Basically smart can fail to detect drive letter stuff from time to time, best to wait a while...
    '''
    # Traceback (most recent call last):
    #   File "C:\Python312\Lib\threading.py", line 1075, in _bootstrap_inner
    #     self.run()
    #   File "C:\Python312\Lib\threading.py", line 1012, in run
    #     self._target(*self._args, **self._kwargs)
    #   File "X:\src\chriscarl.tools.analyze-disk-performance\app.py", line 127, in _telemetry
    #     disk_number = letter_map[drive_letter]
    #                   ~~~~~~~~~~^^^^^^^^^^^^^^
    while True:
        try:
            cdi = lib.crystaldiskinfo()
            letter_map = {value['Drive Letter']: num for num, value in cdi.items()}
            return cdi, letter_map
        except Exception:
            logging.debug('error, trying again in 5 sec...', exc_info=True)
            for _ in range(int(5 * 100)):
                time.sleep(1 / 100)
                if event.is_set():
                    break


def telemetry(
    poll=constants.POLL,
    smart_filepath=constants.DATA_FILEPATH,
    data_filepath=constants.DATA_FILEPATH,
    summary_filepath=constants.SUMMARY_FILEPATH,
    no_crystaldiskinfo=constants.NO_CRYSTALDISKINFO,
    all_drives=constants.ALL_DRIVES,
    log_level=constants.LOG_LEVEL,
    event=threading.Event(),
):
    # type: (float|int, str, str, str, bool, bool, str, threading.Event) -> None
    '''
    Description:
        Poll telemetry including S.M.A.R.T. and others.

    Arguments:
        poll: float|int
            interval between sampling
        data_filepath: str
            the destination of the actual file to be written since we're operating at the OS level
        smart_filepath: str
            where to save the crystaldisinfo S.M.A.R.T. data
        summary_filepath: str
            where to save the final executive summary
        no_crystaldiskinfo: bool
            default False, disable so you can run without admin
        all_drives: bool
            default False, get all drive S.M.A.R.T. data instead of only the drive who hosts the data_filepath

    Returns:
        bytearray
    '''
    logging.basicConfig(
        format='%(asctime)s - %(levelname)10s - %(funcName)48s - %(message)s',
        level=log_level,
        stream=sys.stdout,
        force=True
    )

    logging.debug('polling every %s sec', poll)
    if all_drives:
        drive_letter, _ = os.path.splitdrive(data_filepath)
    else:
        drive_letter = ''

    prior_percent = None
    disk_number = ''

    if not no_crystaldiskinfo:
        cdi, letter_map = _telemetry_smart(event)
        logging.debug(pprint.pformat(letter_map, indent=2))
        if not cdi:
            # we failed and tried multiple times or things were cancelled early during a failure
            return
        cdi_df = pd.DataFrame(cdi.values())
        summary_columns = [key for key in constants.CRYSTAL_KEYS if key in cdi_df.columns]
        logging.debug('\n%s', cdi_df[summary_columns])
        if drive_letter:
            disk_number = letter_map[drive_letter]
        columns = upsert_df_to_csv(cdi_df, smart_filepath)

    iteration = 0
    while not event.is_set():
        for _ in range(int(poll * 1000)):
            time.sleep(1 / 1000)
            if event.is_set():
                break

        logging.debug('poll: %d', iteration)
        if not no_crystaldiskinfo:
            cdi = lib.crystaldiskinfo()
            with open(smart_filepath, 'a', encoding='utf-8', newline='') as a:
                writer = csv.DictWriter(a, fieldnames=columns)
                if disk_number:
                    value = cdi[str(disk_number)]
                    writer.writerow(value)
                    logging.debug('\n%s', pd.DataFrame([{k: v for k, v in value.items() if k in summary_columns}]))
                else:
                    for value in cdi.values():
                        writer.writerow(value)
                    logging.debug(
                        '\n%s',
                        pd.DataFrame(
                            [{
                                k: v
                                for k, v in value.items() if k in summary_columns
                            } for value in cdi.values()]
                        )
                    )

        if drive_letter and disk_number:
            du = psutil.disk_usage(drive_letter)
            if str(du.percent) != str(prior_percent):
                logging.info('disk usage (%s|disk %s): %s%%', drive_letter, disk_number, du.percent)
                prior_percent = str(du.percent)

        iteration += 1

    if not no_crystaldiskinfo:
        cdi = lib.crystaldiskinfo()
        if drive_letter and disk_number:
            df = pd.DataFrame([cdi[str(disk_number)]])
        else:
            df = pd.DataFrame(cdi.values())
        upsert_df_to_csv(df, smart_filepath)
        logging.info('S.M.A.R.T. Telemetry:\n%s', df)

        cdi_df = pd.read_csv(smart_filepath)
        summary_df = cdi_df.iloc[[0, -1]][summary_columns]
        summary_df.to_csv(summary_filepath, index=False)


def crystaldiskinfo_detect():
    # type: () -> int
    try:
        output = subprocess.check_output(
            ['where.exe' if sys.platform == 'win32' else 'which', constants.CRYSTALDISKINFO_EXE],
            universal_newlines=True
        )
        for line in output.splitlines():
            strip = line.strip()
            if os.path.isfile(strip):
                constants.CRYSTALDISKINFO_EXE = strip
                break
        if not os.path.isfile(strip):
            raise OSError(f'Could not find "{constants.CRYSTALDISKINFO_EXE}"!')
    except subprocess.CalledProcessError:
        logging.warning('CrystalDiskInfo not installed or not on path!')
        return 1
    return 0


def admin_detect():
    # type: () -> int
    try:
        admin_ps1 = abspath(SCRIPT_DIRPATH, r"scripts\win32\admin.ps1")
        subprocess.check_call(['powershell', admin_ps1])
        logging.info('admin detected!')
    except subprocess.CalledProcessError:
        logging.warning('not admin!')
        return 1
    return 0


def telemetry_thread(
    poll=constants.POLL,
    smart_filepath=constants.DATA_FILEPATH,
    data_filepath=constants.DATA_FILEPATH,
    summary_filepath=constants.SUMMARY_FILEPATH,
    no_crystaldiskinfo=constants.NO_CRYSTALDISKINFO,
    all_drives=constants.ALL_DRIVES,
    log_level=constants.LOG_LEVEL,
):
    # type: (float|int, str, str, str, bool, bool, str) -> Tuple[threading.Event, threading.Thread]
    logging.basicConfig(
        format='%(asctime)s - %(levelname)10s - %(funcName)48s - %(message)s',
        level=log_level,
        stream=sys.stdout,
        force=True
    )
    logging.debug('checking CrystalDiskInfo access...')
    if not no_crystaldiskinfo:
        if crystaldiskinfo_detect() != 0:
            raise RuntimeError('Cannot run CrystalDiskInfo!')

        logging.debug('checking admin access...')
        if admin_detect() != 0:
            raise RuntimeError('Must be run as administrator or sudo!')

    event = threading.Event()
    t = threading.Thread(
        target=telemetry,
        kwargs=dict(
            event=event,
            smart_filepath=smart_filepath,
            data_filepath=data_filepath,
            summary_filepath=summary_filepath,
            poll=poll,
            no_crystaldiskinfo=no_crystaldiskinfo,
            all_drives=all_drives,
            log_level=log_level,
        )
    )
    t.start()
    return event, t


def upsert_df_to_csv(df, filepath, index=False):
    # type: (pd.DataFrame, str, bool) -> List[str]
    dirpath = os.path.dirname(filepath)
    os.makedirs(dirpath, exist_ok=True)
    if os.path.isfile(filepath):
        old_df = pd.read_csv(filepath)
        new = pd.concat([old_df, df])
        new.to_csv(filepath, index=index)
        return new.columns.tolist()
    else:
        df.to_csv(filepath, index=index)
        return df.columns.tolist()


def fill_till_bursting(byte_array, drive, data_filepath=constants.DATA_FILEPATH, event=threading.Event()):
    # type: (bytearray, str, str, threading.Event) -> None
    # write the bulk of the data
    byte_array_bytes = len(byte_array)
    with open(data_filepath, 'ab') as wb:
        while psutil.disk_usage(drive).free > byte_array_bytes:
            if event.is_set():
                return
            wb.write(byte_array)

        try:
            # write the last chunk in 1mb increments until disk fills and raises OSError
            for i in range(byte_array_bytes // constants.MB):
                if event.is_set():
                    return
                if psutil.disk_usage(drive).free > constants.MB:
                    one_mb_array = byte_array[i * constants.MB:(i + 1) * constants.MB]
                    wb.write(one_mb_array)
                else:
                    break
        except OSError:
            pass  # this is expected behavior


def touch(filepath):
    with open(filepath, 'wb'):  # touch the file
        pass


def write(
    data_filepath=constants.DATA_FILEPATH,
    size=constants.SIZE,
    value=constants.VALUE,
    search_optimal=constants.SEARCH_OPTIMAL,
    search_optimal_filepath=constants.SEARCH_OPTIMAL_FILEPATH,
    iterations=constants.ITERATIONS,
    duration=constants.DURATION,
    burn_in=constants.BURN_IN,
    byte_array=None,
):
    # type: (str, int, int, bool, str, int, float|int, bool, Optional[bytearray]) -> bytearray
    '''
    Description:
        Write a file to the disk, perhaps random, repeatedly, fill the drive, set size, etc.

        >>> write('/tmp/file')  # write random file to fill the disk, loop indefinitely
        >>> write('/tmp/file', duration=10)  # write random file to fill the disk, break if elapsed exceeded, else loop
        >>> write('/tmp/file', value=69, size=1024)  # write 1024 bytes of 64 to the same file infinitely (burn-in)

    Arguments:
        data_filepath: str
            the destination of the actual file to be written since we're operating at the OS level
        size: int
            -1 to fill the disk, else, size in in bytes
            negates search_optimal
        value: int
            -1 for random, else, repeat the same value for all bytes
        search_optimal_filepath: bool
            default true (fill the disk). if true, do some repeated writes to find the "best" size, else just generate
        iterations: int
            -1 for loop infinitely, else exec ends iteration exceeded (or duration exceeded)
        duration: float|int
            -1 for loop infinitely, else exec ends after elapsed exceeded in seconds (or duration exceeded)
        burn_in: bool
            default False, rewrite to the same place, not append and fill
        byte_array: Optional[bytearray]
            use this bytearray instead of generating it (useful when passing one to the other)

    Returns:
        bytearray
    '''
    drive_letter, _ = os.path.splitdrive(data_filepath)
    if byte_array and isinstance(byte_array, bytearray):
        logging.info('reusing bytearray of size %s...', lib.bytes_to_size(len(byte_array)))
    else:
        logging.info('creating bytearray...')
        if size != constants.SIZE:
            byte_array = lib.create_bytearray(size, value=value)
        else:
            byte_array = lib.create_byte_array_high_throughput(
                data_filepath=data_filepath, search_optimal_filepath=search_optimal_filepath, value=value
            )

    filesize = lib.bytes_to_size(len(byte_array))
    until = ''
    if iterations == constants.ITERATIONS and duration == constants.DURATION:
        until = 'infinitely'
    elif duration != constants.DURATION:
        until = f'{duration} secs'
    elif iterations != constants.ITERATIONS:
        until = f'{iterations} iterations'
    summary = (
        f'writing in {"burn-in (write)" if burn_in else "fulpak (append)"} mode, '
        f'on drive {drive_letter}, '
        f'repeatedly using a {filesize} file{"" if burn_in else " in chunks"}, '
        f'running {until}'
    )
    logging.warning(summary)

    try:
        touch(data_filepath)
        iteration = 1
        if iterations > -1:
            for iteration in range(1, iterations + 1):
                logging.info('writing %d / %d', iteration, iterations)
                touch(data_filepath)
                if burn_in:
                    with open(data_filepath, 'wb') as wb:
                        wb.write(byte_array)
                else:
                    fill_till_bursting(byte_array, drive_letter, data_filepath=data_filepath)

        elif duration > -1:
            start = time.time()
            elapsed = time.time() - start
            while elapsed < duration:
                logging.info('writing until %s > %s, iteration %d', elapsed, duration, iteration)
                touch(data_filepath)
                if burn_in:
                    with open(data_filepath, 'wb') as wb:
                        wb.write(byte_array)
                else:
                    # TODO: change architecture to duration and timer based, when the timer goes off, an event is flagged, signalling everyone to halt the fuck up
                    fill_till_bursting(byte_array, drive_letter, data_filepath=data_filepath)
                if elapsed > duration:
                    logging.info('time exceeded,  %s > %s', elapsed, duration)
                    break

                iteration += 1
                elapsed = time.time() - start
        else:
            while True:
                logging.info('writing infinitely, iteration %d', iteration)
                touch(data_filepath)
                if burn_in:
                    with open(data_filepath, 'wb') as wb:
                        wb.write(byte_array)
                else:
                    fill_till_bursting(byte_array, drive_letter, data_filepath=data_filepath)

                iteration += 1
    except KeyboardInterrupt:
        logging.warning('cancelling...')
    finally:
        return byte_array


def read_random(byte_array, data_filepath=constants.DATA_FILEPATH, chunk_size=16):
    # type: (bytearray, str, int) -> bool
    '''
    Description:
        Read a file by randomly jumping around with seek and reads

    Arguments:
        byte_array: Optional[bytearray]
            use this bytearray instead of generating it (useful when passing one to the other)
        data_filepath: str
            the destination of the actual file to be written since we're operating at the OS level
        chunk_size: int
            default -1, MUST evenly divide byte_array length
                instead of sequentially asserting, randomly dart around the file
                say the byte_array length 32, data_filepath length 64, chunks 4
                we will generate 64 / 4 = 16 "windows" to jump around and compare

    Returns:
        bool
    '''
    filesize = os.path.getsize(data_filepath)
    arrsize = len(byte_array)
    if arrsize % chunk_size != 0:
        raise TypeError(
            f'chunks must evenly divide the byte_array size! {arrsize} % {chunk_size} == {arrsize % chunk_size}!'
        )
    # [0, 640, 1280, 1920, 2560, 3200, 3840, 4480, 5120, 5760, 6400, 7040, 7680, 8320, 8960, 9600]
    idxes = list(range(0, filesize, chunk_size))
    # [3840, 9600, 1920, 640, 5760, 4480, 2560, 3200, 5120, 7680, 7040, 6400, 8320, 0, 8960, 1280]
    random.shuffle(idxes)
    bytes_read = 0
    with open(data_filepath, 'rb') as rb:
        prior = ''
        for i, file_idx in enumerate(idxes):
            _ = rb.seek(file_idx)
            read_array = rb.read(chunk_size)

            truth_idx = file_idx % arrsize
            # in case we're at the LAST idx, and didnt read much
            truth_array = byte_array[truth_idx:truth_idx + len(read_array)]
            assert read_array == truth_array, (
                '\n'.join([f'on iteration {i}, full array read != write!'] + lib.diff_bytes(read_array, truth_array))
            )
            bytes_read += chunk_size
            bytes_read_size = bytes_read / constants.MB
            bytes_read_size_str = f'{bytes_read_size:0.1f}'
            print(bytes_read)
            if prior != bytes_read_size_str:
                logging.debug('%0.3f%% or %s MB read', (i + 1) / (len(idxes)) * 100, bytes_read_size_str)
                prior = bytes_read_size_str

    bytes_read_size = bytes_read / constants.MB
    bytes_read_size_str = f'{bytes_read_size:0.3f}'
    logging.debug('%0.3f%% or %s MB read', (i + 1) / (len(idxes)) * 100, bytes_read_size_str)
    prior = bytes_read_size_str

    return True


def read(
    byte_array=None,
    data_filepath=constants.DATA_FILEPATH,
    random_read=constants.RANDOM_READ,
    iterations=constants.ITERATIONS,
    duration=constants.DURATION,
):
    # type: (Optional[bytearray], str, int, int, float|int) -> bool
    '''
    Description:
        Read a file from the disk, perhaps random, repeatedly, etc.

        >>> ba, data_filepath = bytearray(list(range(1024))), '/tmp/file'
        >>> read(byte_array=ba, data_filepath=data_filepath)  # window-read file sequentially, assert match
        >>> read(byte_array=ba, data_filepath=data_filepath, random_read=True)  # generate random windows, assert match

    Arguments:
        byte_array: Optional[bytearray]
            use this bytearray instead of generating it (useful when passing one to the other)
        data_filepath: str
            the destination of the actual file to be written since we're operating at the OS level
        random_read: int
            default -1, MUST evenly divide byte_array length
                instead of sequentially asserting, randomly dart around the file
                say the byte_array length 32, data_filepath length 64, chunks 4
                we will generate 64 / 4 = 16 "windows" to jump around and compare
        iterations: int
            -1 for loop infinitely, else exec ends iteration exceeded (or duration exceeded)
        duration: float|int
            -1 for loop infinitely, else exec ends after elapsed exceeded in seconds (or duration exceeded)

    Returns:
        bool
    '''
    if not isinstance(byte_array, bytearray):
        raise TypeError(f'byte_array must be of type bytearray, provided {type(byte_array)}!')
    filesize = os.path.getsize(data_filepath)
    arrsize = len(byte_array)
    if random_read != constants.RANDOM_READ and arrsize % random_read != 0:
        raise TypeError(
            f'random_read must evenly divide the byte_array size! {arrsize} % {random_read} == {arrsize % random_read}!'
        )

    summary = "random " if random_read != constants.RANDOM_READ else ""
    logging.warning(
        '%sread of file of %s by assertion array of %s', summary, lib.bytes_to_size(filesize),
        lib.bytes_to_size(arrsize)
    )

    iteration = 1
    if iterations > -1:
        for iteration in range(1, iterations + 1):
            logging.info('reading %d / %d', iteration, iterations)
            if random_read == constants.RANDOM_READ:
                if not lib.read_bytearray_from_disk(byte_array, data_filepath=data_filepath):
                    return False
            else:
                if not read_random(byte_array, data_filepath=data_filepath, chunk_size=random_read):
                    return False

    elif duration > -1:
        start = time.time()
        elapsed = time.time() - start
        while elapsed < duration:
            logging.info('reading until %s > %s, iteration %d', elapsed, duration, iteration)
            if random_read == constants.RANDOM_READ:
                if not lib.read_bytearray_from_disk(byte_array, data_filepath=data_filepath):
                    return False
            else:
                if not read_random(byte_array, data_filepath=data_filepath, chunk_size=random_read):
                    return False

            iteration += 1
            elapsed = time.time() - start

    else:
        while True:
            logging.info('reading infinitely, iteration %d', iteration)
            if random_read == constants.RANDOM_READ:
                if not lib.read_bytearray_from_disk(byte_array, data_filepath=data_filepath):
                    return False
            else:
                if not read_random(byte_array, data_filepath=data_filepath, chunk_size=random_read):
                    return False

            iteration += 1

    return True


FUNC_NAMES = ['write', 'read', 'flow', 'telemetry']


def flow_run(
    steps=FUNC_NAMES,
    data_filepath=constants.DATA_FILEPATH,
    size=constants.SIZE,
    value=constants.VALUE,
    search_optimal=constants.SEARCH_OPTIMAL,
    search_optimal_filepath=constants.SEARCH_OPTIMAL_FILEPATH,
    iterations=constants.ITERATIONS,
    duration=constants.DURATION,
    burn_in=constants.BURN_IN,
    byte_array=None,
    random_read=constants.RANDOM_READ,
):
    '''
    Description:
        run other functions in a long line
    '''
    gbls = globals()
    lcls = locals()
    kwargs = {k: lcls[k] for k in OPERATION_ARGUMENTS if k in lcls}
    for s, step in enumerate(steps):
        func = gbls[step]
        logging.info('starting %s / %s - %r', s + 1, len(steps), func.__name__)

        signature = inspect.signature(func)
        subkwargs = {k: kwargs[k] for k in signature.parameters}
        # otherwise its too much to print
        logging.debug(pprint.pformat({k: v for k, v in subkwargs.items() if k not in ['byte_array']}, indent=2))

        res = func(**subkwargs)
        if isinstance(res, bytearray):
            kwargs['byte_array'] = res


def flow(
    steps=FUNC_NAMES,
    flow_iterations=constants.ITERATIONS,
    flow_duration=constants.DURATION,
    # all other kwargs
    data_filepath=constants.DATA_FILEPATH,
    size=constants.SIZE,
    value=constants.VALUE,
    search_optimal=constants.SEARCH_OPTIMAL,
    search_optimal_filepath=constants.SEARCH_OPTIMAL_FILEPATH,
    iterations=constants.ITERATIONS,
    duration=constants.DURATION,
    burn_in=constants.BURN_IN,
    byte_array=None,
    random_read=constants.RANDOM_READ,
):
    '''
    Description:
        run other functions in a long line
    '''
    lcls = locals()
    signature = inspect.signature(flow_run)
    flow_run_kwargs = {k: lcls[k] for k in signature.parameters}

    iteration = 1
    if flow_iterations > -1:
        for iteration in range(1, flow_iterations + 1):
            logging.info('flow %d / %d', iteration, flow_iterations)
            flow_run(**flow_run_kwargs)

    elif flow_duration > -1:
        start = time.time()
        elapsed = time.time() - start
        while elapsed < flow_duration:
            logging.info('flow until %s > %s, iteration %d', elapsed, flow_duration, iteration)
            flow_run(**flow_run_kwargs)

            iteration += 1
            elapsed = time.time() - start

    else:
        while True:
            logging.info('flow infinitely, iteration %d', iteration)
            flow_run(**flow_run_kwargs)

            iteration += 1


FUNCS = [locals()[key] for key in FUNC_NAMES]
OPERATION_ARGUMENTS = {
    'data_filepath': dict(type=str, default=constants.DATA_FILEPATH, help='file that fills the disk'),
    'size': dict(type=int, default=constants.SIZE, help='size in killobytes, so --size * 1024B'),
    'value': dict(type=int, default=constants.VALUE, help='default random, fill with constant value'),
    'search_optimal': dict(type=bool, default=constants.SEARCH_OPTIMAL, help='test different sizes for throughput'),
    'search_optimal_filepath': dict(type=str, default=constants.SEARCH_OPTIMAL_FILEPATH, help='throughput csv'),
    'iterations': dict(type=int, default=constants.ITERATIONS, help='repetitions, -1 for infinitely'),
    'duration': dict(type=float, default=constants.DURATION, help='in seconds, -1 for infinitely'),
    'burn_in': dict(type=bool, help='default False, rewrite to the same place, not append and fill'),
    'steps': dict(type=str, nargs='+', choices=FUNC_NAMES, required=True, help='run funcs in series'),
    'random_read':
        dict(type=int, default=constants.RANDOM_READ, help='default -1, MUST evenly divide size, randomly dart around'),
    'flow_iterations': dict(type=int, default=constants.FLOW_ITERATIONS, help='repetitions, -1 for infinitely'),
    'flow_duration': dict(type=float, default=constants.FLOW_DURATION, help='in seconds, -1 for infinitely'),
    'byte_array': dict(type=bytearray, nargs='+', default=bytearray(), help='BUG: used as part of dynamic programming'),
}
TELEMETRY_ARGUMENTS = {
    'poll': dict(type=float, default=constants.POLL, help='telemetry poll poll'),
    'data_filepath': dict(type=str, default=constants.DATA_FILEPATH, help='file that fills the disk'),
    'smart_filepath': dict(type=str, default=constants.SMART_FILEPATH, help='dump S.M.A.R.T. from CrystalDiskInfo.'),
    'summary_filepath': dict(type=str, default=constants.SUMMARY_FILEPATH, help='afteraction summary'),
    'log_level': dict(type=str, default=constants.LOG_LEVEL, choices=constants.LOG_LEVELS, help='log level'),
    'no_crystaldiskinfo': dict(type=bool, help='if disabled, you can run without admin!'),
    'all_drives': dict(type=bool, help='if enabled, it queries telemetry from all drives, rather than the one'),
}


def validate_kwargs(
    func=write,
    log_level='INFO',
    value=constants.VALUE,
    size=constants.SIZE,
    duration=constants.DURATION,
    flow_duration=constants.FLOW_DURATION,
    iterations=constants.ITERATIONS,
    flow_iterations=constants.FLOW_ITERATIONS,
    data_filepath=constants.DATA_FILEPATH,
    perf_filepath=constants.PERF_FILEPATH,
    search_optimal_filepath=constants.SEARCH_OPTIMAL_FILEPATH,
    summary_filepath=constants.SUMMARY_FILEPATH,
    smart_filepath=constants.SMART_FILEPATH,
    poll=constants.POLL,
    random_read=constants.RANDOM_READ,
    ignore_partitions=None,
    steps=None,
    byte_array=None,
    search_optimal=constants.SEARCH_OPTIMAL,
    no_optimizations=False,
    burn_in=constants.BURN_IN,
    no_crystaldiskinfo=constants.NO_CRYSTALDISKINFO,
    all_drives=constants.ALL_DRIVES,
):
    if func not in FUNCS:
        raise KeyError(f'func {func!r} does not exist, use one of {FUNCS}!')
    if log_level not in constants.LOG_LEVELS:
        raise KeyError(f'log_level {log_level!r} does not exist!')
    if value != -1:
        if value < 0 and 255 < value:
            raise ValueError('value must be a value between [0,255] or -1')
    if size < 1:
        raise ValueError('size must be a postive int, are you nuts?')
    if duration < 0 and duration != -1:
        raise ValueError('duration must be a postive num (or -1), are you nuts?')
    if flow_duration < 0 and flow_duration != -1:
        raise ValueError('flow_duration must be a postive num (or -1), are you nuts?')
    if iterations < -1:
        raise ValueError('iterations must be a postive num (or -1), are you nuts?')
    if flow_iterations < -1:
        raise ValueError('flow_iterations must be a postive num (or -1), are you nuts?')
    for filepath in [data_filepath, perf_filepath, search_optimal_filepath, summary_filepath, smart_filepath]:
        if not os.path.isdir(os.path.dirname(filepath)):
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
    if isinstance(ignore_partitions, list):
        for ignore_partition in ignore_partitions:
            if ignore_partition not in string.ascii_uppercase:
                raise ValueError(f'ignore_partition {ignore_partition!r} not in expected possibilities!')
    if poll < 0:
        raise ValueError('poll must be positive!')
    if random_read != constants.RANDOM_READ and random_read < 0:
        raise ValueError('random_read must be positive!')

    steps = steps or []
    if not isinstance(steps, list):
        raise TypeError(f'steps must be of type bool, provided {type(steps)}')
    assert all(step in FUNC_NAMES for step in steps), f'not all steps provided are real, only these are: {FUNC_NAMES}'
    byte_array = byte_array or bytearray()
    if not isinstance(byte_array, bytearray):
        raise TypeError(f'byte_array must be of type bytearray, provided {type(byte_array)}')

    if not isinstance(no_optimizations, bool):
        raise TypeError(f'no_optimizations must be of type bool, provided {type(no_optimizations)}')
    if not isinstance(search_optimal, bool):
        raise TypeError(f'search_optimal must be of type bool, provided {type(search_optimal)}')
    if not isinstance(burn_in, bool):
        raise TypeError(f'burn_in must be of type bool, provided {type(burn_in)}')
    if not isinstance(no_crystaldiskinfo, bool):
        raise TypeError(f'no_crystaldiskinfo must be of type bool, provided {type(no_crystaldiskinfo)}')
    if not isinstance(all_drives, bool):
        raise TypeError(f'all_drives must be of type bool, provided {type(all_drives)}')


def main():
    parser = argparse.ArgumentParser(prog=constants.APP_NAME, description=__doc__, formatter_class=NiceFormatter)
    operations = parser.add_subparsers(help='different operations we can do')
    for func in FUNCS:
        op = operations.add_parser(
            func.__name__,
            help=func.__doc__.strip().splitlines()[1].strip(),  # just under Description:
            description=func.__doc__,
            formatter_class=NiceFormatter,
        )
        op.set_defaults(func=func)
        group = op.add_argument_group('specific')
        signature = inspect.signature(func)
        for key in signature.parameters:
            kwargs = OPERATION_ARGUMENTS.get(key)
            if not kwargs:
                continue
            names = []
            if '_' in key:
                names.append(f'--{key.replace("_", "-")}')
                names.append(f'--{key}')
            else:
                names.append(f'--{key}')
            if kwargs['type'] is bool:
                group.add_argument(
                    *names, action='store_true', **{
                        key: value
                        for key, value in kwargs.items() if key != 'type'
                    }
                )
            else:
                group.add_argument(*names, **kwargs)

        group = op.add_argument_group('telemetry')
        signature = inspect.signature(telemetry)
        for key in signature.parameters:
            if key in OPERATION_ARGUMENTS:
                continue
            kwargs = TELEMETRY_ARGUMENTS.get(key)
            if not kwargs:
                continue
            names = []
            if '_' in key:
                names.append(f'--{key.replace("_", "-")}')
                names.append(f'--{key}')
            else:
                names.append(f'--{key}')
            if kwargs['type'] is bool:
                group.add_argument(
                    *names, action='store_true', **{
                        key: value
                        for key, value in kwargs.items() if key != 'type'
                    }
                )
            else:
                group.add_argument(*names, **kwargs)

    args = parser.parse_args()
    func = args.func
    logging.basicConfig(
        format='%(asctime)s - %(levelname)10s - %(funcName)48s - %(message)s', level=args.log_level, stream=sys.stdout
    )

    kwargs = vars(args)
    validate_kwargs(**kwargs)
    telemetry_kwargs = {k: kwargs[k] for k in TELEMETRY_ARGUMENTS}
    if func != telemetry:
        operation_kwargs = {k: kwargs[k] for k in OPERATION_ARGUMENTS}
    else:
        operation_kwargs = telemetry_kwargs

    logging.info('starting %r', func.__name__)
    logging.debug(pprint.pformat(kwargs, indent=2))
    stop_event = threading.Event()
    success = True
    stop_event, thread = None, None
    try:
        if func != telemetry:
            stop_event, thread = telemetry_thread(**telemetry_kwargs)

        logging.info('starting %r', func.__name__)
        func(**operation_kwargs)
    except KeyboardInterrupt:
        logging.warning('ctrl + c detected!')
    except Exception as ex:
        logging.debug('ERROR: exception encountered during execution!', exc_info=True)
        logging.error(str(ex))
        success = False
    finally:
        if stop_event:
            stop_event.set()
            thread.join()
        if success:
            logging.info('success!')
        else:
            logging.error('failure!')


if __name__ == '__main__':
    main()
