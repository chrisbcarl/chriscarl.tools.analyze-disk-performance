# encoding: utf-8
'''
The MIT License (MIT)
Copyright © 2023 Chris Carl <chrisbcarl@outlook.com>
Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the “Software”), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all
  copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.

Author:     Chris Carl <chrisbcarl@outlook.com>
Date:       2024-09-26
Modified:   2024-09-26

Modified:
    2025-08-07 - chrisbcarl - added create_partitions, delete_partitions
                                added health, now I just need to test
                                fine tuning after running it successfully, added defaults awareness
    2025-08-06 - chrisbcarl - nearly full re-write, re-orged into app.py, will be further developed on
    2025-08-05 - chrisbcarl - moved files to the root of the perf filepath so the files
                                dont contend with each other for bandwidth or space
    2025-08-03 - chrisbcarl - moved functions around, perfected the read_bytearray_from_disk
                              cleaned up perf+fill+read minutia
    2025-08-02 - chrisbcarl - added perf+fill+read and smartmon
    2025-08-01 - chrisbcarl - changed to chriscarl.tools.analyze-disk-performance
                              created the 'health' module which is designed to rip through all disks
                              added crystaldiskinfo utilization, we'll see how it goes after testing.
    2024-09-28 - chrisbcarl - added "write" mode, much easier.
    2024-09-26 - chrisbcarl - complete rewrite with different modes this time
    2023-07-05 - chrisbcarl - init commit, behaves like sequential write/readback, thats it.

TODO:
operations:
    departition
    partition

files to keep track of
    smart performance csv
    create-fast performance csv
    summary csv

    - move toward list operations rather than explicitly perf + write, like you can mix perf write loop etc, implying
        the bytearray generated by perf is used by write and loop
    - deal with the situation where create_bytearray would result in a bytearray the size larger than the universe.
        better would be to make some object that when you ask for an index or the next byte,
        it GENRATES it, is writable, etc...
    - dynamic crystaldiskinfo txt

Examples:
    - What most people think of as write-read benchmarking
        python main.py perf+fill+read --data-filepath Y:/temp
    - Find performance sweetspot and fill the disk at partition D:/
        python main.py perf+fill --data-filepath D:/temp
    - Evaluate overall health on all newly inserted disks
        python main.py health --ignore-partitions C --log-level DEBUG
    - Just launch a cyrstaldiskinfo monitor
        python main.py smartmon
    - Write a 1kb file of 69s 10 times
        python app.py write --size 1024 --value 69 --data-filepath Y:/tmp --iterations 10
'''
# stdlib
from __future__ import print_function, division
import os
import sys
import csv
import time
import json
import pprint
import string
import random
import logging
import inspect
import argparse
import datetime
import threading
import subprocess
# import re
# import copy
# import multiprocessing
from typing import Tuple, Dict, List, Optional

# 3rd party
import numpy as np
import pandas as pd
import psutil

# app imports
import lib
import constants
from stdlib import NiceFormatter, abspath

SCRIPT_DIRPATH = os.path.abspath(os.path.dirname(__file__))


def _telemetry_smart(event):
    # type: (threading.Event) -> Tuple[Dict[str, dict], dict]
    '''
    Basically smart can fail to detect drive letter stuff from time to time, best to wait a while...
    '''
    # Traceback (most recent call last):
    #   File "C:\Python312\Lib\threading.py", line 1075, in _bootstrap_inner
    #     self.run()
    #   File "C:\Python312\Lib\threading.py", line 1012, in run
    #     self._target(*self._args, **self._kwargs)
    #   File "X:\src\chriscarl.tools.analyze-disk-performance\app.py", line 127, in _telemetry
    #     disk_number = letter_map[drive_letter]
    #                   ~~~~~~~~~~^^^^^^^^^^^^^^
    while True:
        try:
            cdi = lib.crystaldiskinfo()
            letter_map = {value['Drive Letter']: num for num, value in cdi.items()}
            return cdi, letter_map
        except Exception:
            logging.debug('error, trying again in 5 sec...', exc_info=True)
            for _ in range(int(5 * 100)):
                time.sleep(1 / 100)
                if event.is_set():
                    break


def summarize_crystaldiskinfo_df(df):
    df['datetime'] = pd.to_datetime(df['datetime'])
    df['Read Perf'] = pd.Series([''] * len(df))
    df['Write Perf'] = pd.Series([''] * len(df))
    ilocs = []
    rows = []
    for grouping, group_df in df.groupby(['Serial Number']):
        dts = group_df['datetime']
        dt_min, dt_max = dts.min(), dts.max()
        iloc_min = dts.index[dts.argmin()]
        iloc_max = dts.index[dts.argmax()]
        ilocs += [iloc_min, iloc_max]
        elapsed = (dt_max - dt_min).total_seconds()

        reads = group_df['Host Reads']
        reads_min, reads_max = reads.min(), reads.max()
        read_throughput, write_throughput = '?', '?'
        if reads_min == np.nan:
            reads_min = -1
            reads_min = -1
        elif isinstance(reads_min, (int, float)):
            pass
        else:
            unit = reads_min.split()[-1]
            reads_min, reads_max = reads_min.split()[0], reads_max.split()[0]
        reads_min, reads_max = float(reads_min), float(reads_max)

        writes = group_df['Host Writes']
        writes_min, writes_max = writes.min(), writes.max()
        if writes_min == np.nan:
            writes_min = -1
            writes_max = -1
        elif isinstance(writes_min, (int, float)):
            pass
        else:
            writes_min, writes_max = writes_min.split()[0], writes_max.split()[0]
        writes_min, writes_max = float(writes_min), float(writes_max)

        if elapsed == 0:
            read_throughput = f'0 {unit}/s'
            write_throughput = f'0 {unit}/s'
        else:
            read_throughput = f'{(reads_max - reads_min) / elapsed:0.3f} {unit}/s'
            write_throughput = f'{(writes_max - writes_min) / elapsed:0.3f} {unit}/s'
        df.loc[group_df.index, 'Read Perf'] = read_throughput
        df.loc[group_df.index, 'Write Perf'] = write_throughput

        drive_letter = df.iloc[iloc_max]['Drive Letter']
        disk_size = df.iloc[iloc_max]['Disk Size'].split()
        disk_size, disk_unit = disk_size[0], disk_size[1]
        serial = df.iloc[iloc_max]['Serial Number']
        health = df.iloc[iloc_max]['Health Status']
        disk_number = df.iloc[iloc_max]['Disk Number']
        pcs = df.iloc[iloc_max]['Power On Count'].split()[0]
        xfer = df.iloc[iloc_max]['Transfer Mode'].split(' | ')[-1]  # pcie
        text = (
            f'{serial} ({disk_size} {disk_unit}) | {disk_number} | {drive_letter} | '
            f'{health} | Reads: {read_throughput} | Writes: {read_throughput} | '
        )
        row = dict(
            serial=serial,
            size=f'{disk_size} {disk_unit}',
            number=disk_number,
            letter=drive_letter,
            health=health,
            elapsed=f'{elapsed / 3600:0.2f}hrs',
            reads=f'{reads_max - reads_min} {disk_unit}',
            writes=f'{writes_max - writes_min} {disk_unit}',
            read_bw=read_throughput,
            write_bw=write_throughput,
        )
        for col in constants.CRYSTAL_ERROR_KEYS:
            if col not in df.columns:
                continue
            errs = group_df[col]
            err_min, err_max = errs.min(), errs.max()
            if pd.isna(err_min):
                err_min = 0
            if pd.isna(err_max):
                err_max = 0
            err_delta = err_max - err_min
            row[f'{col} (Delta)'] = err_delta

        rows.append(row)

    df = pd.DataFrame(rows)
    return df


def telemetry(
    skip_telemetry=constants.SKIP_TELEMETRY,
    poll=constants.POLL,
    smart_filepath=constants.SMART_FILEPATH,
    data_filepath=constants.DATA_FILEPATH,
    summary_filepath=constants.SUMMARY_FILEPATH,
    no_crystaldiskinfo=constants.NO_CRYSTALDISKINFO,
    all_drives=constants.ALL_DRIVES,
    log_level=constants.LOG_LEVEL,
    event=threading.Event(),
):
    # type: (bool, float|int, str, str, str, bool, bool, str, threading.Event) -> None
    '''
    Description:
        Poll telemetry including S.M.A.R.T. and others.

    Arguments:
        skip_telemetry: bool
            short circuit exit
        poll: float|int
            interval between sampling
        data_filepath: str
            the destination of the actual file to be written since we're operating at the OS level
        smart_filepath: str
            where to save the crystaldisinfo S.M.A.R.T. data
        summary_filepath: str
            where to save the final executive summary
        no_crystaldiskinfo: bool
            default False, disable so you can run without admin
        all_drives: bool
            default False, get all drive S.M.A.R.T. data instead of only the drive who hosts the data_filepath

    Returns:
        bytearray
    '''
    logging.basicConfig(
        format='%(asctime)s - %(levelname)10s - %(funcName)48s - %(message)s',
        level=log_level,
        stream=sys.stdout,
        force=True
    )
    if skip_telemetry:
        logging.warning('skipping telemetry!')
        return

    logging.debug('polling every %s sec', poll)
    if all_drives:
        drive_letter, _ = os.path.splitdrive(data_filepath)
    else:
        drive_letter = ''

    prior_percent = None
    disk_number = ''

    if not no_crystaldiskinfo:
        cdi, letter_map = _telemetry_smart(event)
        logging.debug(pprint.pformat(letter_map, indent=2))
        if not cdi:
            # we failed and tried multiple times or things were cancelled early during a failure
            return
        cdi_df = pd.DataFrame(cdi.values())
        summary_columns = [key for key in constants.CRYSTAL_KEYS if key in cdi_df.columns]
        logging.debug('\n%s', cdi_df[summary_columns])
        if drive_letter:
            disk_number = letter_map[drive_letter]
        columns = upsert_df_to_csv(cdi_df, smart_filepath)

    iteration = 0
    while not event.is_set():
        for _ in range(int(poll * 1000)):
            time.sleep(1 / 1000)
            if event.is_set():
                break

        logging.debug('poll: %d', iteration)
        if not no_crystaldiskinfo:
            cdi = lib.crystaldiskinfo()
            with open(smart_filepath, 'a', encoding='utf-8', newline='') as a:
                writer = csv.DictWriter(a, fieldnames=columns)
                if disk_number:
                    value = cdi[str(disk_number)]
                    writer.writerow(value)
                    logging.debug('\n%s', pd.DataFrame([{k: v for k, v in value.items() if k in summary_columns}]))
                else:
                    for value in cdi.values():
                        writer.writerow(value)
                    logging.debug(
                        '\n%s',
                        pd.DataFrame(
                            [{
                                k: v
                                for k, v in value.items() if k in summary_columns
                            } for value in cdi.values()]
                        )
                    )

        if drive_letter and disk_number:
            du = psutil.disk_usage(drive_letter)
            if str(du.percent) != str(prior_percent):
                logging.info('disk usage (%s|disk %s): %s%%', drive_letter, disk_number, du.percent)
                prior_percent = str(du.percent)

        iteration += 1

    if not no_crystaldiskinfo:
        cdi = lib.crystaldiskinfo()
        if drive_letter and disk_number:
            df = pd.DataFrame([cdi[str(disk_number)]])
        else:
            df = pd.DataFrame(cdi.values())
        upsert_df_to_csv(df, smart_filepath)
        logging.info('S.M.A.R.T. Telemetry:\n%s', df)

        cdi_df = pd.read_csv(smart_filepath)

        summary_df = summarize_crystaldiskinfo_df(cdi_df)
        summary_df.to_csv(summary_filepath, index=False)


def crystaldiskinfo_detect():
    # type: () -> int
    try:
        output = subprocess.check_output(
            ['where.exe' if sys.platform == 'win32' else 'which', constants.CRYSTALDISKINFO_EXE],
            universal_newlines=True
        )
        for line in output.splitlines():
            strip = line.strip()
            if os.path.isfile(strip):
                constants.CRYSTALDISKINFO_EXE = strip
                break
        if not os.path.isfile(strip):
            raise OSError(f'Could not find "{constants.CRYSTALDISKINFO_EXE}"!')
    except subprocess.CalledProcessError:
        logging.warning('CrystalDiskInfo not installed or not on path!')
        return 1
    return 0


def admin_detect():
    # type: () -> int
    try:
        admin_ps1 = abspath(SCRIPT_DIRPATH, r"scripts\win32\admin.ps1")
        subprocess.check_call(['powershell', admin_ps1])
        logging.info('admin detected!')
    except subprocess.CalledProcessError:
        logging.warning('not admin!')
        return 1
    return 0


def telemetry_thread(
    skip_telemetry=constants.SKIP_TELEMETRY,
    poll=constants.POLL,
    smart_filepath=constants.DATA_FILEPATH,
    data_filepath=constants.DATA_FILEPATH,
    summary_filepath=constants.SUMMARY_FILEPATH,
    no_crystaldiskinfo=constants.NO_CRYSTALDISKINFO,
    all_drives=constants.ALL_DRIVES,
    log_level=constants.LOG_LEVEL,
):
    # type: (bool, float|int, str, str, str, bool, bool, str) -> Tuple[Optional[threading.Event], Optional[threading.Thread]]
    logging.basicConfig(
        format='%(asctime)s - %(levelname)10s - %(funcName)48s - %(message)s',
        level=log_level,
        stream=sys.stdout,
        force=True
    )
    logging.debug('checking CrystalDiskInfo access...')
    if not no_crystaldiskinfo:
        if crystaldiskinfo_detect() != 0:
            raise RuntimeError('Cannot run CrystalDiskInfo!')

        logging.debug('checking admin access...')
        if admin_detect() != 0:
            raise RuntimeError('Must be run as administrator or sudo!')

    event = threading.Event()
    t = threading.Thread(
        target=telemetry,
        kwargs=dict(
            skip_telemetry=skip_telemetry,
            event=event,
            smart_filepath=smart_filepath,
            data_filepath=data_filepath,
            summary_filepath=summary_filepath,
            poll=poll,
            no_crystaldiskinfo=no_crystaldiskinfo,
            all_drives=all_drives,
            log_level=log_level,
        )
    )
    t.start()
    return event, t


def upsert_df_to_csv(df, filepath, index=False):
    # type: (pd.DataFrame, str, bool) -> List[str]
    dirpath = os.path.dirname(filepath)
    os.makedirs(dirpath, exist_ok=True)
    if os.path.isfile(filepath):
        old_df = pd.read_csv(filepath)
        new = pd.concat([old_df, df])
        new.to_csv(filepath, index=index)
        return new.columns.tolist()
    else:
        df.to_csv(filepath, index=index)
        return df.columns.tolist()


def fill_till_bursting(byte_array, drive, data_filepath=constants.DATA_FILEPATH, event=threading.Event()):
    # type: (bytearray, str, str, threading.Event) -> None
    # write the bulk of the data
    size = len(byte_array)
    prior = ''
    with open(data_filepath, 'ab') as wb:
        while psutil.disk_usage(drive).free > size:
            if event.is_set():
                return
            wb.write(byte_array)

            getsize = os.path.getsize(data_filepath) / constants.GB
            getsizestr = f'{getsize:0.1f}'
            if getsizestr != prior and int(getsize) % 16 == 0:  # really slow it down
                logging.info('%s GB written', getsizestr)
                prior = f'{getsize:0.1f}'

        try:
            # write the last chunk in 1mb increments until disk fills and raises OSError
            for i in range(size // constants.MB):
                if event.is_set():
                    return
                if psutil.disk_usage(drive).free > constants.MB:
                    one_mb_array = byte_array[i * constants.MB:(i + 1) * constants.GB]
                    wb.write(one_mb_array)
                else:
                    break
        except OSError:
            pass  # this is expected behavior

    remainder = size - os.path.getsize(data_filepath)
    wb.write(byte_array[0:remainder])
    logging.info('%0.3f GB written', os.path.getsize(data_filepath) / constants.GB)


def touch(filepath):
    with open(filepath, 'wb'):  # touch the file
        pass


def write(
    data_filepath=constants.DATA_FILEPATH,
    size=constants.SIZE,
    value=constants.VALUE,
    search_optimal=constants.SEARCH_OPTIMAL,
    search_optimal_filepath=constants.SEARCH_OPTIMAL_FILEPATH,
    iterations=constants.ITERATIONS,
    duration=constants.DURATION,
    burn_in=constants.BURN_IN,
    byte_array=None,
):
    # type: (str, int, int, bool, str, int, float|int, bool, Optional[bytearray]) -> bytearray
    '''
    Description:
        Write a file to the disk, perhaps random, repeatedly, fill the drive, set size, etc.

        >>> write('/tmp/file')  # write random file to fill the disk, loop indefinitely
        >>> write('/tmp/file', duration=10)  # write random file to fill the disk, break if elapsed exceeded, else loop
        >>> write('/tmp/file', value=69, size=1024)  # write 1024 bytes of 64 to the same file infinitely (burn-in)

    Arguments:
        data_filepath: str
            the destination of the actual file to be written since we're operating at the OS level
        size: int
            -1 to fill the disk, else, size in in bytes
            negates search_optimal
        value: int
            -1 for random, else, repeat the same value for all bytes
        search_optimal_filepath: bool
            default true (fill the disk). if true, do some repeated writes to find the "best" size, else just generate
        iterations: int
            -1 for loop infinitely, else exec ends iteration exceeded (or duration exceeded)
        duration: float|int
            -1 for loop infinitely, else exec ends after elapsed exceeded in seconds (or duration exceeded)
        burn_in: bool
            default False, rewrite to the same place, not append and fill
        byte_array: Optional[bytearray]
            use this bytearray instead of generating it (useful when passing one to the other)

    Returns:
        bytearray
    '''
    drive_letter, _ = os.path.splitdrive(data_filepath)
    if search_optimal:
        logging.info('creating bytearray...')
        byte_array = lib.create_byte_array_high_throughput(
            data_filepath=data_filepath, search_optimal_filepath=search_optimal_filepath, value=value
        )
    create = False
    if isinstance(byte_array, bytearray):
        if len(byte_array) == 0:
            create = True
        logging.info('reusing bytearray of size %s...', lib.bytes_to_size(len(byte_array)))
    else:
        create = True
    if create:
        logging.info('creating bytearray...')
        if size != constants.SIZE:
            byte_array = lib.create_bytearray(size, value=value)
        else:
            byte_array = lib.create_byte_array_high_throughput(
                data_filepath=data_filepath, search_optimal_filepath=search_optimal_filepath, value=value
            )
    byte_array = byte_array or bytearray()  # for type hinting

    filesize = lib.bytes_to_size(len(byte_array))
    until = ''
    if iterations == constants.ITERATIONS and duration == constants.DURATION:
        until = 'infinitely'
    elif duration != constants.DURATION:
        until = f'{duration} secs'
    elif iterations != constants.ITERATIONS:
        until = f'{iterations} iterations'
    summary = (
        f'writing in {"burn-in (write)" if burn_in else "fulpak (append)"} mode, '
        f'on drive {drive_letter}, '
        f'repeatedly using a {filesize} file{"" if burn_in else " in chunks"}, '
        f'running {until}'
    )
    logging.warning(summary)

    try:
        touch(data_filepath)
        iteration = 1
        if iterations > -1:
            for iteration in range(1, iterations + 1):
                logging.info('writing %d / %d', iteration, iterations)
                touch(data_filepath)
                if burn_in:
                    with open(data_filepath, 'wb') as wb:
                        wb.write(byte_array)
                else:
                    fill_till_bursting(byte_array, drive_letter, data_filepath=data_filepath)

        elif duration > -1:
            start = time.time()
            elapsed = time.time() - start
            while elapsed < duration:
                logging.info('writing until %s > %s, iteration %d', elapsed, duration, iteration)
                touch(data_filepath)
                if burn_in:
                    with open(data_filepath, 'wb') as wb:
                        wb.write(byte_array)
                else:
                    # TODO: change architecture to duration and timer based, when the timer goes off,
                    #   an event is flagged, signalling everyone to halt the fuck up
                    fill_till_bursting(byte_array, drive_letter, data_filepath=data_filepath)
                if elapsed > duration:
                    logging.info('time exceeded,  %s > %s', elapsed, duration)
                    break

                iteration += 1
                elapsed = time.time() - start
        else:
            while True:
                logging.info('writing infinitely, iteration %d', iteration)
                touch(data_filepath)
                if burn_in:
                    with open(data_filepath, 'wb') as wb:
                        wb.write(byte_array)
                else:
                    fill_till_bursting(byte_array, drive_letter, data_filepath=data_filepath)

                iteration += 1
    except KeyboardInterrupt:
        logging.warning('cancelling...')
    finally:
        return byte_array


def read_random(byte_array, data_filepath=constants.DATA_FILEPATH, chunk_size=16):
    # type: (bytearray, str, int) -> bool
    '''
    Description:
        Read a file by randomly jumping around with seek and reads

    Arguments:
        byte_array: Optional[bytearray]
            use this bytearray instead of generating it (useful when passing one to the other)
        data_filepath: str
            the destination of the actual file to be written since we're operating at the OS level
        chunk_size: int
            default -1, MUST evenly divide byte_array length
                instead of sequentially asserting, randomly dart around the file
                say the byte_array length 32, data_filepath length 64, chunks 4
                we will generate 64 / 4 = 16 "windows" to jump around and compare

    Returns:
        bool
    '''
    filesize = os.path.getsize(data_filepath)
    arrsize = len(byte_array)
    if arrsize % chunk_size != 0:
        raise TypeError(
            f'chunks must evenly divide the byte_array size! {arrsize} % {chunk_size} == {arrsize % chunk_size}!'
        )
    # [0, 640, 1280, 1920, 2560, 3200, 3840, 4480, 5120, 5760, 6400, 7040, 7680, 8320, 8960, 9600]
    idxes = list(range(0, filesize, chunk_size))
    # [3840, 9600, 1920, 640, 5760, 4480, 2560, 3200, 5120, 7680, 7040, 6400, 8320, 0, 8960, 1280]
    random.shuffle(idxes)
    bytes_read = 0
    with open(data_filepath, 'rb') as rb:
        prior = ''
        for i, file_idx in enumerate(idxes):
            _ = rb.seek(file_idx)
            read_array = rb.read(chunk_size)

            truth_idx = file_idx % arrsize
            # in case we're at the LAST idx, and didnt read much
            truth_array = byte_array[truth_idx:truth_idx + len(read_array)]
            assert read_array == truth_array, (
                '\n'.join([f'on iteration {i}, full array read != write!'] + lib.diff_bytes(read_array, truth_array))
            )
            bytes_read += chunk_size
            bytes_read_size = bytes_read / constants.GB
            bytes_read_size_str = str(int(bytes_read_size))
            # print(bytes_read)
            if prior != bytes_read_size_str and int(getsize) % 16 == 0:  # really slow it down
                logging.info('%0.1f%% or %s GB read', (i + 1) / (len(idxes)) * 100, bytes_read_size_str)
                prior = bytes_read_size_str

    bytes_read_size = bytes_read / constants.GB
    bytes_read_size_str = f'{bytes_read_size:0.3f}'
    logging.info('%0.1f%% or %s GB read', (i + 1) / (len(idxes)) * 100, bytes_read_size_str)
    prior = bytes_read_size_str

    return True


def read(
    byte_array=None,
    data_filepath=constants.DATA_FILEPATH,
    random_read=constants.RANDOM_READ,
    iterations=constants.ITERATIONS,
    duration=constants.DURATION,
):
    # type: (Optional[bytearray], str, int, int, float|int) -> bool
    '''
    Description:
        Read a file from the disk, perhaps random, repeatedly, etc.

        >>> ba, data_filepath = bytearray(list(range(1024))), '/tmp/file'
        >>> read(byte_array=ba, data_filepath=data_filepath)  # window-read file sequentially, assert match
        >>> read(byte_array=ba, data_filepath=data_filepath, random_read=True)  # generate random windows, assert match

    Arguments:
        byte_array: Optional[bytearray]
            use this bytearray instead of generating it (useful when passing one to the other)
        data_filepath: str
            the destination of the actual file to be written since we're operating at the OS level
        random_read: int
            default -1, MUST evenly divide byte_array length
                instead of sequentially asserting, randomly dart around the file
                say the byte_array length 32, data_filepath length 64, chunks 4
                we will generate 64 / 4 = 16 "windows" to jump around and compare
        iterations: int
            -1 for loop infinitely, else exec ends iteration exceeded (or duration exceeded)
        duration: float|int
            -1 for loop infinitely, else exec ends after elapsed exceeded in seconds (or duration exceeded)

    Returns:
        bool
    '''
    if not isinstance(byte_array, bytearray):
        raise TypeError(f'byte_array must be of type bytearray, provided {type(byte_array)}!')
    filesize = os.path.getsize(data_filepath)
    arrsize = len(byte_array)
    if random_read != constants.RANDOM_READ and arrsize % random_read != 0:
        raise TypeError(
            f'random_read must evenly divide the byte_array size! {arrsize} % {random_read} == {arrsize % random_read}!'
        )

    summary = "random " if random_read != constants.RANDOM_READ else ""
    logging.warning(
        '%sread of file of %s by assertion array of %s', summary, lib.bytes_to_size(filesize),
        lib.bytes_to_size(arrsize)
    )

    iteration = 1
    if iterations > -1:
        for iteration in range(1, iterations + 1):
            logging.info('reading %d / %d', iteration, iterations)
            if random_read == constants.RANDOM_READ:
                if not lib.read_bytearray_from_disk(byte_array, data_filepath=data_filepath):
                    return False
            else:
                if not read_random(byte_array, data_filepath=data_filepath, chunk_size=random_read):
                    return False

    elif duration > -1:
        start = time.time()
        elapsed = time.time() - start
        while elapsed < duration:
            logging.info('reading until %s > %s, iteration %d', elapsed, duration, iteration)
            if random_read == constants.RANDOM_READ:
                if not lib.read_bytearray_from_disk(byte_array, data_filepath=data_filepath):
                    return False
            else:
                if not read_random(byte_array, data_filepath=data_filepath, chunk_size=random_read):
                    return False

            iteration += 1
            elapsed = time.time() - start

    else:
        while True:
            logging.info('reading infinitely, iteration %d', iteration)
            if random_read == constants.RANDOM_READ:
                if not lib.read_bytearray_from_disk(byte_array, data_filepath=data_filepath):
                    return False
            else:
                if not read_random(byte_array, data_filepath=data_filepath, chunk_size=random_read):
                    return False

            iteration += 1

    return True


def delete_partitions(ignore_partitions=constants.IGNORE_PARTITIONS, include_partitions=None):
    # type: (List[str], Optional[List[str]]) -> List[str]
    '''
    Description:
        Look through all partitions and remove them so the disks are raw

    Arguments:
        ignore_partitions: List[str]
            Ex) ['C']
            If you know of partitions youd like to avoid ahead of time, maybe avoid deleting them...
        include_partitions: List[str]
            If you know which to delete, delete only those, override ignore_partitions

    Returns:
        List[str]
            list of disk numbers as readable by CrystalDiskMark and Windows Disk Utility
    '''
    if admin_detect() != 0:
        raise RuntimeError('Must be run as administrator or sudo!')

    disk_numbers = []
    if not include_partitions:
        # get all partitions
        read_partitions_ps1 = abspath(SCRIPT_DIRPATH, r"scripts\win32\read-partitions.ps1")
        cmd = ['powershell', read_partitions_ps1]
        logging.debug(subprocess.list2cmdline(cmd))
        output = subprocess.check_output(cmd, universal_newlines=True)
        read_partitions = json.loads(output)
        logging.info('partitions identified: %s', json.dumps(read_partitions, indent=2))

        # filter out all partitions that dont belong
        include_partitions = [key for key in read_partitions if key not in ignore_partitions]
        logging.debug('removing drive letters: %s', include_partitions)
        disk_numbers = [val['DiskNumber'] for key, val in read_partitions.items() if key not in ignore_partitions]
        logging.info('disk numbers to be removed after filtering %s: %s', ignore_partitions, disk_numbers)

    if include_partitions:
        # remove partitions so they return to raw
        delete_partitions_ps1 = abspath(SCRIPT_DIRPATH, r"scripts\win32\delete-partitions.ps1")
        cmd = ['powershell', delete_partitions_ps1, '-DriveLetters', ','.join(include_partitions)]
        logging.debug(subprocess.list2cmdline(cmd))
        output = subprocess.check_output(cmd, universal_newlines=True)
        logging.debug(output)
    if disk_numbers:
        logging.info('deleted partitions. disk numbers: %s', disk_numbers)
        return disk_numbers

    return disk_numbers


def create_partitions(disk_numbers=constants.DISK_NUMBERS):
    # type: (List[int]) -> Dict[str, str]
    '''
    Description:
        Go through any raw disk and give them a partition, returning a dict of number: letter

    Arguments:
        disk_numbers: List[int]
            Ex) [0]
            If you know the disk numbers ahead of time, provide them
            else, any raw disk will be partitioned and assigned a drive letter

    Returns:
        Dict[str, str]
            dict of number: letter
            ex) {"1": "D:"}
    '''
    if admin_detect() != 0:
        raise RuntimeError('Must be run as administrator or sudo!')

    # scan for RAW and make new partitions
    create_partitions_ps1 = abspath(SCRIPT_DIRPATH, r"scripts\win32\create-partitions.ps1")
    cmd = ['powershell', create_partitions_ps1]
    if disk_numbers:
        cmd += ['-DiskNumbers', ','.join(str(ele) for ele in disk_numbers)]
    logging.debug(subprocess.list2cmdline(cmd))
    output = subprocess.check_output(cmd, universal_newlines=True)
    create_partitions_sentinel = "Begin Output Parsing Here:"
    logging.debug(output)
    output = output[output.find(create_partitions_sentinel) + len(create_partitions_sentinel) + 1:].strip()
    logging.debug(output)
    disk_number_to_letter_dict = json.loads(output)
    logging.info('created new partitions: %s!', disk_number_to_letter_dict)
    for k, v in disk_number_to_letter_dict.items():
        if v is None:
            raise RuntimeError(f'Drive {k} was unable to create a partition: {v}!')

    return disk_number_to_letter_dict


def health(
    # delete
    ignore_partitions=constants.IGNORE_PARTITIONS,
    include_partitions=None,
    # array
    size=constants.SIZE,
    value=constants.VALUE,
    search_optimal_filepath=constants.SEARCH_OPTIMAL_FILEPATH,
    # flow
    iterations=3,
    duration=constants.DURATION,
    # write
    burn_in=constants.BURN_IN,
    # read
    random_read=constants.RANDOM_READ,
    # general/telemetry
    log_level='INFO',
    poll=constants.POLL,
):
    '''
    Description:
        Launch a pre-determined flow upon every relevant disk. WARNING: DO NOT RUN IN HIGHLY POPULATED PCs!
    '''
    operation = 'write+read'
    logging.info('starting %r', operation)

    if admin_detect() != 0:
        raise RuntimeError('Must be run as administrator or sudo!')

    logging.info('deleting partitions...')
    disk_numbers = delete_partitions(ignore_partitions=ignore_partitions, include_partitions=include_partitions)

    logging.info('creating partitions...')
    disk_number_to_letter_dict = create_partitions(disk_numbers=disk_numbers)
    if disk_numbers and (len(disk_numbers) != len(disk_number_to_letter_dict)):
        raise RuntimeError(
            f'Number of disks != number of partitions created! {len(disk_numbers)} != {len(disk_number_to_letter_dict)} | {disk_number_to_letter_dict}'
        )

    popens = []
    logging.info('flowing on %d partitions/drives...', len(disk_number_to_letter_dict))
    started = datetime.datetime.now()
    output_dirpath = constants.TEMP_DIRPATH
    for drive_number, drive_letter in disk_number_to_letter_dict.items():
        data_filepath = abspath(f'{drive_letter}:/{drive_number}-{operation}.dat')
        search_optimal_filepath = abspath(f'{output_dirpath}/{drive_number}-{operation}-search_optimal.csv')
        stdout = abspath(f'{output_dirpath}/{drive_number}-{operation}.stdout')
        cmd = [
            sys.executable,
            abspath(__file__),
            # flow control
            'flow',
            '--steps',
            'write',
            'read',
            '--flow-iterations',
            iterations,
            '--flow-duration',
            duration,
            # general / telemetry
            '--skip-telemetry',
            '--data-filepath',
            data_filepath,
            '--log-level',
            log_level,
            # array
            '--search-optimal-filepath',
            search_optimal_filepath,
            # read/write args
            '--iterations',
            1
        ]

        # array
        if size == constants.SIZE:
            cmd += ['--search-optimal']
        else:
            cmd += ['--size', size]
        if value != constants.VALUE:
            cmd += ['--value', value]

        if burn_in:
            cmd += ['--burn-in']
        if random_read != constants.RANDOM_READ:
            cmd += ['--random-read', random_read]
        cmd = [str(ele) for ele in cmd]
        logging.debug('drive %s (%s): %s', drive_number, drive_letter, subprocess.list2cmdline(cmd))
        with open(stdout, 'wb') as sout:
            popen = subprocess.Popen(cmd, stdout=sout)
            popens.append(popen)

    try:
        while True:
            now = datetime.datetime.now()
            logging.info('elapsed: %s', now - started)
            if all([popen.poll() is not None for popen in popens]):
                logging.info('All %r finished!', operation)
                break
            time.sleep(poll)
    except KeyboardInterrupt:
        logging.warning('ctrl + c detected! killing processes, removing resources...')

    logging.info('closing resources...')
    for popen in popens:
        if popen.poll() is None:
            popen.kill()
            subprocess.Popen(['taskkill', '/pid', str(popen.pid), '/f', '/t'], shell=True).wait()
    for drive_number, drive_letter in disk_number_to_letter_dict.items():
        data_filepath = abspath(f'{drive_letter}:/{drive_number}-{operation}.dat')
        if os.path.isfile(data_filepath):
            try:
                os.remove(data_filepath)
            except Exception:
                logging.error('unable to delete ""%s', data_filepath)

    # TODO: remove
    # logging.info('removing partitions...')
    # delete_partitions(include_partitions=list(disk_number_to_letter_dict.values()))


FUNC_NAMES = ['write', 'read', 'flow', 'telemetry', 'delete_partitions', 'create_partitions', 'health']


def flow_run(
    steps=FUNC_NAMES,
    data_filepath=constants.DATA_FILEPATH,
    size=constants.SIZE,
    value=constants.VALUE,
    search_optimal=constants.SEARCH_OPTIMAL,
    search_optimal_filepath=constants.SEARCH_OPTIMAL_FILEPATH,
    iterations=constants.ITERATIONS,
    duration=constants.DURATION,
    burn_in=constants.BURN_IN,
    byte_array=None,
    random_read=constants.RANDOM_READ,
):
    '''
    Description:
        run other functions in a long line
    '''
    gbls = globals()
    lcls = locals()
    kwargs = {k: lcls[k] for k in OPERATION_ARGUMENTS if k in lcls}
    for s, step in enumerate(steps):
        func = gbls[step]
        logging.info('starting %s / %s - %r', s + 1, len(steps), func.__name__)

        signature = inspect.signature(func)
        subkwargs = {k: kwargs[k] for k in signature.parameters}
        # otherwise its too much to print
        logging.debug(pprint.pformat({k: v for k, v in subkwargs.items() if k not in ['byte_array']}, indent=2))

        res = func(**subkwargs)
        if isinstance(res, bytearray):
            kwargs['byte_array'] = res
        elif func == delete_partitions:
            kwargs['disk_numbers'] = res
        elif func == create_partitions:
            kwargs['disk_number_to_letter_dict'] = res


def flow(
    steps=FUNC_NAMES,
    flow_iterations=constants.ITERATIONS,
    flow_duration=constants.DURATION,
    # all other kwargs
    data_filepath=constants.DATA_FILEPATH,
    size=constants.SIZE,
    value=constants.VALUE,
    search_optimal=constants.SEARCH_OPTIMAL,
    search_optimal_filepath=constants.SEARCH_OPTIMAL_FILEPATH,
    iterations=constants.ITERATIONS,
    duration=constants.DURATION,
    burn_in=constants.BURN_IN,
    byte_array=None,
    random_read=constants.RANDOM_READ,
):
    '''
    Description:
        run other functions in a long line
    '''
    lcls = locals()
    signature = inspect.signature(flow_run)
    flow_run_kwargs = {k: lcls[k] for k in signature.parameters}

    iteration = 1
    if flow_iterations > -1:
        for iteration in range(1, flow_iterations + 1):
            logging.info('flow %d / %d', iteration, flow_iterations)
            flow_run(**flow_run_kwargs)

    elif flow_duration > -1:
        start = time.time()
        elapsed = time.time() - start
        while elapsed < flow_duration:
            logging.info('flow until %s > %s, iteration %d', elapsed, flow_duration, iteration)
            flow_run(**flow_run_kwargs)

            iteration += 1
            elapsed = time.time() - start

    else:
        while True:
            logging.info('flow infinitely, iteration %d', iteration)
            flow_run(**flow_run_kwargs)

            iteration += 1


_lcls = locals()
FUNCS = [_lcls[key] for key in FUNC_NAMES]
OPERATION_ARGUMENTS = {
    'data_filepath': dict(type=str, default=constants.DATA_FILEPATH, help='file that fills the disk'),
    'size': dict(type=int, default=constants.SIZE, help='size in killobytes, so --size * 1024B'),
    'value': dict(type=int, default=constants.VALUE, help='default random, fill with constant value'),
    'search_optimal': dict(type=bool, default=constants.SEARCH_OPTIMAL, help='test different sizes for throughput'),
    'search_optimal_filepath': dict(type=str, default=constants.SEARCH_OPTIMAL_FILEPATH, help='throughput csv'),
    'iterations': dict(type=int, default=constants.ITERATIONS, help='repetitions, -1 for infinitely'),
    'duration': dict(type=float, default=constants.DURATION, help='in seconds, -1 for infinitely'),
    'burn_in': dict(type=bool, help='default False, rewrite to the same place, not append and fill'),
    'steps': dict(type=str, nargs='+', choices=FUNC_NAMES, required=True, help='run funcs in series'),
    'random_read':
        dict(type=int, default=constants.RANDOM_READ, help='default -1, MUST evenly divide size, randomly dart around'),
    'flow_iterations': dict(type=int, default=constants.FLOW_ITERATIONS, help='repetitions, -1 for infinitely'),
    'flow_duration': dict(type=float, default=constants.FLOW_DURATION, help='in seconds, -1 for infinitely'),
    'byte_array': dict(type=bytearray, nargs='+', default=bytearray(), help='BUG: used as part of dynamic programming'),
    'ignore_partitions':
        dict(type=str, nargs='*', default=constants.IGNORE_PARTITIONS, help='if known partitions, ignore these'),
    'include_partitions': dict(type=str, nargs='*', default=[], help='override ignore, include only these'),
    'disk_numbers': dict(type=int, nargs='*', default=constants.DISK_NUMBERS, help='if known disk numbers, use these'),
    'disk_number_to_letter_dict': dict(type=str, help='pass as string, ex) {"1": "D:"}'),
}
TELEMETRY_ARGUMENTS = {
    'skip_telemetry': dict(type=bool, help='skip telemetry entirely'),
    'all_drives': dict(type=bool, help='if enabled, it queries telemetry from all drives, rather than the one'),
    'poll': dict(type=float, default=constants.POLL, help='telemetry poll poll'),
    'no_crystaldiskinfo': dict(type=bool, help='if disabled, you can run without admin!'),
    'data_filepath': dict(type=str, default=constants.DATA_FILEPATH, help='file that fills the disk'),
    'smart_filepath': dict(type=str, default=constants.SMART_FILEPATH, help='dump S.M.A.R.T. from CrystalDiskInfo.'),
    'summary_filepath': dict(type=str, default=constants.SUMMARY_FILEPATH, help='afteraction summary'),
    'log_level': dict(type=str, default=constants.LOG_LEVEL, choices=constants.LOG_LEVELS, help='log level'),
}


def validate_kwargs(
    args,
    func=write,
    log_level='INFO',
    value=constants.VALUE,
    size=constants.SIZE,
    duration=constants.DURATION,
    flow_duration=constants.FLOW_DURATION,
    iterations=constants.ITERATIONS,
    flow_iterations=constants.FLOW_ITERATIONS,
    data_filepath=constants.DATA_FILEPATH,
    search_optimal_filepath=constants.SEARCH_OPTIMAL_FILEPATH,
    summary_filepath=constants.SUMMARY_FILEPATH,
    smart_filepath=constants.SMART_FILEPATH,
    poll=constants.POLL,
    random_read=constants.RANDOM_READ,
    # lists
    steps=None,
    byte_array=None,
    ignore_partitions=None,
    include_partitions=None,
    disk_numbers=None,
    # dicts
    disk_number_to_letter_dict=None,
    # bools
    search_optimal=constants.SEARCH_OPTIMAL,
    no_optimizations=False,
    burn_in=constants.BURN_IN,
    no_crystaldiskinfo=constants.NO_CRYSTALDISKINFO,
    all_drives=constants.ALL_DRIVES,
    skip_telemetry=constants.SKIP_TELEMETRY,
):
    if func not in FUNCS:
        raise KeyError(f'func {func!r} does not exist, use one of {FUNCS}!')
    if log_level not in constants.LOG_LEVELS:
        raise KeyError(f'log_level {log_level!r} does not exist!')
    if value != -1:
        if value < 0 and 255 < value:
            raise ValueError('value must be a value between [0,255] or -1')
    if size != -1 and size <= 0:
        raise ValueError('size must be a postive int, are you nuts?')
    if duration < 0 and duration != -1:
        raise ValueError('duration must be a postive num (or -1), are you nuts?')
    if flow_duration < 0 and flow_duration != -1:
        raise ValueError('flow_duration must be a postive num (or -1), are you nuts?')
    if iterations < -1:
        raise ValueError('iterations must be a postive num (or -1), are you nuts?')
    if flow_iterations < -1:
        raise ValueError('flow_iterations must be a postive num (or -1), are you nuts?')
    for filepath in [data_filepath, search_optimal_filepath, summary_filepath, smart_filepath]:
        if not os.path.isdir(os.path.dirname(filepath)):
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
    if poll < 0:
        raise ValueError('poll must be positive!')
    if random_read != constants.RANDOM_READ and random_read < 0:
        raise ValueError('random_read must be positive!')

    # lists
    steps = steps or []
    if not isinstance(steps, list):
        raise TypeError(f'steps must be of type bool, provided {type(steps)}')
    assert all(step in FUNC_NAMES for step in steps), f'not all steps provided are real, only these are: {FUNC_NAMES}'
    byte_array = byte_array or bytearray()
    if not isinstance(byte_array, bytearray):
        raise TypeError(f'byte_array must be of type bytearray, provided {type(byte_array)}')
    if isinstance(ignore_partitions, list):
        for i, ignore_partition in enumerate(ignore_partitions):
            if ignore_partition not in string.ascii_uppercase:
                raise ValueError(f'ignore_partition {i + 1} {ignore_partition!r} not in expected possibilities!')
    if isinstance(include_partitions, list):
        for i, include_partition in enumerate(include_partitions):
            if include_partition not in string.ascii_uppercase:
                raise ValueError(f'include_partition {i + 1} {include_partition!r} not in expected possibilities!')
    if isinstance(disk_numbers, list):
        for i, disk_number in enumerate(disk_numbers):
            fail = False
            try:
                int(disk_number)
            except ValueError:
                fail = True
            if fail:
                raise ValueError(f'disk_number {i + 1} is not an int!')

    # dicts
    if disk_number_to_letter_dict:
        disk_number_to_letter_dict = json.loads(disk_number_to_letter_dict)
        args.disk_number_to_letter_dict = disk_number_to_letter_dict

    # bools
    if not isinstance(no_optimizations, bool):
        raise TypeError(f'no_optimizations must be of type bool, provided {type(no_optimizations)}')
    if not isinstance(search_optimal, bool):
        raise TypeError(f'search_optimal must be of type bool, provided {type(search_optimal)}')
    if not isinstance(burn_in, bool):
        raise TypeError(f'burn_in must be of type bool, provided {type(burn_in)}')
    if not isinstance(no_crystaldiskinfo, bool):
        raise TypeError(f'no_crystaldiskinfo must be of type bool, provided {type(no_crystaldiskinfo)}')
    if not isinstance(all_drives, bool):
        raise TypeError(f'all_drives must be of type bool, provided {type(all_drives)}')
    if not isinstance(skip_telemetry, bool):
        raise TypeError(f'skip_telemetry must be of type bool, provided {type(skip_telemetry)}')


def main():
    parser = argparse.ArgumentParser(prog=constants.APP_NAME, description=__doc__, formatter_class=NiceFormatter)
    operations = parser.add_subparsers(help='different operations we can do')
    for func in FUNCS:
        op = operations.add_parser(
            func.__name__,
            help=func.__doc__.strip().splitlines()[1].strip(),  # just under Description:
            description=func.__doc__,
            formatter_class=NiceFormatter,
        )
        op.set_defaults(func=func)
        group = op.add_argument_group('specific')
        signature = inspect.signature(func)
        for key in signature.parameters:
            # print(func, key)
            argparse_kwargs = {k: v for k, v in OPERATION_ARGUMENTS.get(key, {}).items()}  # copy
            if 'default' in argparse_kwargs:
                default = argparse_kwargs['default']
                func_default = signature.parameters[key].default
                if default != func_default:
                    argparse_kwargs['default'] = func_default
            if not argparse_kwargs:
                continue
            names = []
            if '_' in key:
                names.append(f'--{key.replace("_", "-")}')
                names.append(f'--{key}')
            else:
                names.append(f'--{key}')
            if argparse_kwargs['type'] is bool:
                group.add_argument(
                    *names,
                    action='store_true',
                    **{
                        key: value
                        for key, value in argparse_kwargs.items() if key != 'type'
                    }
                )
            else:
                group.add_argument(*names, **argparse_kwargs)

        group = op.add_argument_group('telemetry')
        signature = inspect.signature(telemetry)
        for key in signature.parameters:
            # print(func, key)
            # if key in OPERATION_ARGUMENTS:
            #     continue
            argparse_kwargs = {k: v for k, v in TELEMETRY_ARGUMENTS.get(key, {}).items()}  # copy
            if 'default' in argparse_kwargs:
                default = argparse_kwargs['default']
                func_default = signature.parameters[key].default
                if default != func_default:
                    argparse_kwargs['default'] = func_default
            if not argparse_kwargs:
                continue
            names = []
            if '_' in key:
                names.append(f'--{key.replace("_", "-")}')
                names.append(f'--{key}')
            else:
                names.append(f'--{key}')
            try:
                if argparse_kwargs['type'] is bool:
                    group.add_argument(
                        *names,
                        action='store_true',
                        **{
                            key: value
                            for key, value in argparse_kwargs.items() if key != 'type'
                        }
                    )
                else:
                    group.add_argument(*names, **argparse_kwargs)
            except argparse.ArgumentError as ae:
                aestr = str(ae)
                if 'conflicting option strings' in aestr:
                    pass

    args = parser.parse_args()
    func = args.func
    logging.basicConfig(
        format='%(asctime)s - %(levelname)10s - %(funcName)48s - %(message)s', level=args.log_level, stream=sys.stdout
    )

    kwargs = vars(args)
    logging.debug(pprint.pformat(kwargs, indent=2))
    validate_kwargs(args, **kwargs)

    telemetry_signature = inspect.signature(telemetry)
    telemetry_kwargs = {k: kwargs[k] for k in telemetry_signature.parameters if k in TELEMETRY_ARGUMENTS}
    telemetry_kwargs.update({k: kwargs[k] for k in telemetry_signature.parameters if k in OPERATION_ARGUMENTS})

    func_signature = inspect.signature(func)
    func_kwargs = {k: kwargs[k] for k in func_signature.parameters if k in OPERATION_ARGUMENTS}
    func_kwargs.update({k: kwargs[k] for k in func_signature.parameters if k in TELEMETRY_ARGUMENTS})

    logging.info('starting %r', func.__name__)
    logging.debug(pprint.pformat(kwargs, indent=2))
    stop_event = threading.Event()
    success = True
    stop_event, thread = None, None
    try:
        if func not in [telemetry, create_partitions, delete_partitions]:
            stop_event, thread = telemetry_thread(**telemetry_kwargs)

        logging.info('starting %r', func.__name__)
        func(**func_kwargs)
    except KeyboardInterrupt:
        logging.warning('ctrl + c detected!')
    except Exception as ex:
        logging.debug('ERROR: exception encountered during execution!', exc_info=True)
        logging.error(str(ex))
        success = False
    finally:
        if stop_event:
            stop_event.set()
            thread.join()
        if success:
            logging.info('success!')
        else:
            logging.error('failure!')


if __name__ == '__main__':
    main()
